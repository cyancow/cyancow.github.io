



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://docs.prometheus.cool/storage/ceph/">
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="jp">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../assets/img/prometheus_logo.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.0.2">
    
    
      
        <title>Ceph - 云原生监控神器Prometheus</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.982221ab.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../../assets/javascripts/modernizr.1f0bcf2b.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700|Ubuntu+Mono">
        <style>body,input{font-family:"Ubuntu","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../../assets/styles/extra.css">
    
      <link rel="stylesheet" href="../../assets/styles/prism.css">
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#ceph" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://docs.prometheus.cool" title="云原生监控神器Prometheus" class="md-header-nav__button md-logo">
          
            <img src="../../assets/img/prometheus_logo.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              云原生监控神器Prometheus
            </span>
            <span class="md-header-nav__topic">
              Ceph
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/k8stech/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://docs.prometheus.cool" title="云原生监控神器Prometheus" class="md-nav__button md-logo">
      
        <img src="../../assets/img/prometheus_logo.png" width="48" height="48">
      
    </a>
    云原生监控神器Prometheus
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/k8stech/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      监控基础与概述
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        监控基础与概述
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../monitorbasic/whymonitor/" title="为什么要监控" class="md-nav__link">
      为什么要监控
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../monitorbasic/monitorterms/" title="监控术语" class="md-nav__link">
      监控术语
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../monitorbasic/metricstory/" title="指标物语" class="md-nav__link">
      指标物语
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../monitorbasic/monitorcontrast/" title="开源监控对比" class="md-nav__link">
      开源监控对比
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Prometheus 基础
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Prometheus 基础
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../basic/prom-introduction/" title="Prometheus介绍" class="md-nav__link">
      Prometheus介绍
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../basic/tsdb-contrast/" title="时序数据库对比" class="md-nav__link">
      时序数据库对比
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../basic/prom-datamodel/" title="数据模型" class="md-nav__link">
      数据模型
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../basic/node-exporter/" title="Node_Exporter" class="md-nav__link">
      Node_Exporter
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../basic/prom-config/" title="Prometheus安装与配置" class="md-nav__link">
      Prometheus安装与配置
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Prometheus 进阶
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Prometheus 进阶
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../advanced/auto-ops-exporter-1/" title="自动化维护Exporter（一）" class="md-nav__link">
      自动化维护Exporter（一）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../advanced/auto-ops-exporter-2/" title="自动化维护Exporter（二）" class="md-nav__link">
      自动化维护Exporter（二）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../advanced/docker-swarm-monitor-1/" title="Docker-Swarm集群监控（一）" class="md-nav__link">
      Docker-Swarm集群监控（一）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../advanced/docker-swarm-monitor-2/" title="Docker-Swarm集群监控（二）" class="md-nav__link">
      Docker-Swarm集群监控（二）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../advanced/commonly-exporter/" title="常用Exporter介绍与配置" class="md-nav__link">
      常用Exporter介绍与配置
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Prometheus(警报)
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Prometheus(警报)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/promql-basic/" title="PromQL详解（一）" class="md-nav__link">
      PromQL详解（一）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/promql-operator/" title="PromQL详解（二）" class="md-nav__link">
      PromQL详解（二）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/promql-operator2/" title="PromQL详解（三）" class="md-nav__link">
      PromQL详解（三）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/promql-function/" title="PromQL函数" class="md-nav__link">
      PromQL函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/alertmanager-overview/" title="AlertManager" class="md-nav__link">
      AlertManager
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/alertmanager-rules-1/" title="Rules详解（一）" class="md-nav__link">
      Rules详解（一）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/alertmanager-rules-2/" title="Rules详解（二）" class="md-nav__link">
      Rules详解（二）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/alertmanager-receiver/" title="Receiver配置" class="md-nav__link">
      Receiver配置
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Alertmanager/alertmanager-silences/" title="Silences配置" class="md-nav__link">
      Silences配置
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Prometheus(联邦集群)
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Prometheus(联邦集群)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../federation/federation-overview/" title="Prometheus 联邦集群" class="md-nav__link">
      Prometheus 联邦集群
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../federation/pushgateway/" title="Pushgateway 代理" class="md-nav__link">
      Pushgateway 代理
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../federation/alertmanager-ha/" title="Alertmanager 高可用" class="md-nav__link">
      Alertmanager 高可用
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7">
    
    <label class="md-nav__link" for="nav-7">
      Prometheus(服务发现)
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        Prometheus(服务发现)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../discover-service/discovery-overview/" title="服务发现（文件、DNS）" class="md-nav__link">
      服务发现（文件、DNS）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../discover-service/discovery-relabeling/" title="服务发现（Relabelling）" class="md-nav__link">
      服务发现（Relabelling）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../discover-service/discovery-consul/" title="服务发现（Consul）" class="md-nav__link">
      服务发现（Consul）
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8">
    
    <label class="md-nav__link" for="nav-8">
      Prometheus(Operator)
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        Prometheus(Operator)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Kubernetes/Prometheus-Operator/" title="Operator概述" class="md-nav__link">
      Operator概述
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Kubernetes/Prometheus-Statefulsets-1/" title="手动部署（1）" class="md-nav__link">
      手动部署（1）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Kubernetes/Prometheus-Statefulsets-2/" title="手动部署（2）" class="md-nav__link">
      手动部署（2）
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Kubernetes/Prometheus-Statefulsets-3/" title="手动部署（联邦）" class="md-nav__link">
      手动部署（联邦）
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" title="简介" class="md-nav__link">
    简介
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" title="架构" class="md-nav__link">
    架构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" title="组件" class="md-nav__link">
    组件
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" title="块存储" class="md-nav__link">
    块存储
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" title="文件存储" class="md-nav__link">
    文件存储
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="对象存储" class="md-nav__link">
    对象存储
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_7" title="部署" class="md-nav__link">
    部署
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" title="环境" class="md-nav__link">
    环境
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" title="安装" class="md-nav__link">
    安装
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="验证" class="md-nav__link">
    验证
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dashboard" title="Dashboard" class="md-nav__link">
    Dashboard
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_11" title="使用" class="md-nav__link">
    使用
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/k8stech/edit/master/docs/storage/ceph.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="ceph">Ceph<a class="headerlink" href="#ceph" title="Permanent link">&para;</a></h1>
<p><a href="https://ceph.io/">Ceph</a> 是一个统一的分布式存储系统，提供较好的性能、可靠性和可扩展性。最早起源于 Sage 博士期间的工作，随后贡献给开源社区。</p>
<h2 id="_1">简介<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p><strong>高性能</strong></p>
<ul>
<li>抛弃了传统的集中式存储运输局寻址的方案，采用 <code>CRUSH</code> 算法，数据分布均衡，并行度高。</li>
<li>考虑了容灾域的隔离，能够实现各类负载的副本设置规则，例如跨机房、机架感知等。</li>
<li>能够支持上千个存储节点的规模，支持 TB 到 PB 级的数据。</li>
</ul>
<p><strong>高可用性</strong></p>
<ul>
<li>副本数可以灵活控制</li>
<li>支持故障域分离，数据强一致性</li>
<li>多种故障场景自动进行修复自愈</li>
<li>没有单点故障，自动管理</li>
</ul>
<p><strong>高可扩展性</strong></p>
<ul>
<li>去中心化</li>
<li>扩展灵活</li>
<li>随着节点增加而线性增长</li>
</ul>
<p><strong>特性丰富</strong></p>
<ul>
<li>支持三种存储接口：块存储、文件存储、对象存储</li>
<li>支持自定义接口，支持多种语言驱动</li>
</ul>
<h3 id="_2">架构<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<p><strong>支持三种接口</strong></p>
<ul>
<li>Object：有原生 API，而且也兼容 Swift 和 S3 的 API</li>
<li>Block：支持精简配置、快照、克隆</li>
<li>File：Posix 接口，支持快照</li>
</ul>
<p><img alt="ceph rados" src="../../assets/img/storage/ceph-rados.png" /></p>
<h3 id="_3">组件<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p><strong>Monitor</strong>：一个 Ceph 集群需要多个 Monitor 组成的小集群，它们通过 Paxos 同步数据，用来保存 OSD 的元数据。</p>
<p><strong>OSD</strong>：全称 <code>Object Storage Device</code>，也就是负责响应客户端请求返回具体数据的进程，一个 Ceph 集群一般都有很多个 OSD。主要功能用于数据的存储，当直接使用硬盘作为存储目标时，一块硬盘称之为 <code>OSD</code>，当使用一个目录作为存储目标的时候，这个目录也被称为 <code>OSD</code>。</p>
<p><strong>MDS</strong>：全称 <code>Ceph Metadata Server</code>，是 CephFS 服务依赖的元数据服务，对象存储和块设备存储不需要该服务。</p>
<p><strong>Object</strong>：Ceph 最底层的存储单元是 Object 对象，一条数据、一个配置都是一个对象，每个 Object 包含 ID、元数据和原始数据。</p>
<p><strong>Pool</strong>：Pool 是一个存储对象的逻辑分区，它通常规定了数据冗余的类型与副本数，默认为3副本。对于不同类型的存储，需要单独的 Pool，如 RBD。</p>
<p><strong>PG</strong>：全称 <code>Placement Grouops</code>，是一个逻辑概念，一个 OSD 包含多个 PG。引入 PG 这一层其实是为了更好的分配数据和定位数据。每个 <code>Pool</code> 内包含很多个 PG，它是一个对象的集合，服务端数据均衡和恢复的最小单位就是 PG。</p>
<p><img alt="ceph pool pg" src="../../assets/img/storage/ceph-pool-pg.png" /></p>
<ul>
<li>pool 是 ceph 存储数据时的逻辑分区，它起到 namespace 的作用</li>
<li>每个 pool 包含一定数量(可配置)的 PG</li>
<li>PG 里的对象被映射到不同的 Object 上</li>
<li>pool 是分布到整个集群的</li>
</ul>
<p><strong>FileStore与BlueStore</strong>：FileStore 是老版本默认使用的后端存储引擎，如果使用 FileStore，建议使用 <code>xfs</code> 文件系统。BlueStore 是一个新的后端存储引擎，可以直接管理裸硬盘，抛弃了 ext4 与 xfs 等本地文件系统。可以直接对物理硬盘进行操作，同时效率也高出很多。</p>
<p><strong>RADOS</strong>：全称 <code>Reliable Autonomic Distributed Object Store</code>，是 Ceph 集群的精华，用于实现数据分配、Failover 等集群操作。</p>
<p><strong>Librados</strong>：<code>Librados</code> 是 Rados 提供库，因为 RADOS 是协议很难直接访问，因此上层的 RBD、RGW 和 CephFS 都是通过 librados 访问的，目前提供 PHP、Ruby、Java、Python、C 和 C++ 支持。</p>
<p><strong>CRUSH</strong>：<code>CRUSH</code> 是 Ceph 使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。</p>
<p><strong>RBD</strong>：全称 <code>RADOS Block Device</code>，是 Ceph 对外提供的块设备服务，如虚拟机硬盘，支持快照功能。</p>
<p><strong>RGW</strong>：全称是 <code>RADOS Gateway</code>，是 Ceph 对外提供的对象存储服务，接口与 S3 和 Swift 兼容。</p>
<p><strong>CephFS</strong>：全称 <code>Ceph File System</code>，是 Ceph 对外提供的文件系统服务。</p>
<h2 id="_4">块存储<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p><strong>典型设备</strong></p>
<p>磁盘阵列，硬盘，主要是将裸磁盘空间映射给主机使用的。</p>
<p><strong>优点</strong></p>
<ul>
<li>通过 Raid 与 LVM 等手段，对数据提供了保护。</li>
<li>多块廉价的硬盘组合起来，提高容量。</li>
<li>多块磁盘组合出来的逻辑盘，提升读写效率。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>采用 SAN 架构组网时，光纤交换机，造价成本高。</li>
<li>主机之间无法共享数据。</li>
</ul>
<p><strong>使用场景</strong></p>
<ul>
<li>Docker 容器、虚拟机磁盘存储分配。</li>
<li>日志存储</li>
<li>文件存储</li>
<li>...</li>
</ul>
<h3 id="_5">文件存储<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p><strong>典型设备</strong>
FTP、NFS 服务器，为了克服块存储文件无法共享的问题，所以有了文件存储，在服务器上架设 FTP 与 NFS 服务器，就是文件存储。</p>
<p><strong>优点</strong></p>
<ul>
<li>造价低，随便一台机器就可以了</li>
<li>方便文件可以共享</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>读写速率低</li>
<li>传输速率慢</li>
</ul>
<p><strong>使用场景</strong></p>
<ul>
<li>日志存储</li>
<li>有目录结构的文件存储</li>
<li>...</li>
</ul>
<h3 id="_6">对象存储<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p><strong>典型设备</strong></p>
<p>内置大容量硬盘的分布式服务器(swift, s3)；多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。</p>
<p><strong>优点</strong></p>
<ul>
<li>具备块存储的读写高速。</li>
<li>具备文件存储的共享等特性</li>
</ul>
<p><strong>使用场景</strong>：(适合更新变动较少的数据)</p>
<ul>
<li>图片存储</li>
<li>视频存储</li>
<li>...</li>
</ul>
<h2 id="_7">部署<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<p>由于我们这里在 Kubernetes 集群中使用，也为了方便管理，我们这里使用 Rook 来部署 Ceph 集群，<a href="https://rook.io/">Rook</a> 是一个开源的云原生存储编排工具，提供平台、框架和对各种存储解决方案的支持，以和云原生环境进行本地集成。</p>
<p>Rook 将存储软件转变成自我管理、自我扩展和自我修复的存储服务，通过自动化部署、启动、配置、供应、扩展、升级、迁移、灾难恢复、监控和资源管理来实现。Rook 底层使用云原生容器管理、调度和编排平台提供的能力来提供这些功能，其实就是我们平常说的 Operator。Rook 利用扩展功能将其深度集成到云原生环境中，并为调度、生命周期管理、资源管理、安全性、监控等提供了无缝的体验。有关 Rook 当前支持的存储解决方案的状态的更多详细信息，可以参考 <a href="https://github.com/rook/rook/blob/master/README.md#project-status">Rook 仓库</a> 的项目介绍。</p>
<p><img alt="rook ceph" src="../../assets/img/storage/rook-k8s.png" /></p>
<p>Rook 包含多个组件：</p>
<ul>
<li>Rook Operator：Rook 的核心组件，Rook Operator 是一个简单的容器，自动启动存储集群，并监控存储守护进程，来确保存储集群的健康。</li>
<li>Rook Agent：在每个存储节点上运行，并配置一个 FlexVolume 或者 CSI 插件，和 Kubernetes 的存储卷控制框架进行集成。Agent 处理所有的存储操作，例如挂接网络存储设备、在主机上加载存储卷以及格式化文件系统等。</li>
<li>Rook Discovers：检测挂接到存储节点上的存储设备。</li>
</ul>
<p>Rook 还会用 Kubernetes Pod 的形式，部署 Ceph 的 MON、OSD 以及 MGR 守护进程。Rook Operator 让用户可以通过 CRD 来创建和管理存储集群。每种资源都定义了自己的 CRD：</p>
<ul>
<li>RookCluster：提供了对存储机群的配置能力，用来提供块存储、对象存储以及共享文件系统。每个集群都有多个 Pool。</li>
<li>Pool：为块存储提供支持，Pool 也是给文件和对象存储提供内部支持。</li>
<li>Object Store：用 S3 兼容接口开放存储服务。</li>
<li>File System：为多个 Kubernetes Pod 提供共享存储。</li>
</ul>
<h3 id="_8">环境<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<p>Rook Ceph 需要使用 RBD 内核模块，我们可以通过运行 <code>modprobe rbd</code> 来测试 Kubernetes 节点是否有该模块，如果没有，则需要更新下内核版本。</p>
<p>另外需要在节点上安装 <code>lvm2</code> 软件包：
<pre class="highlight"><code class="language-shell"># Centos
sudo yum install -y lvm2

# Ubuntu
sudo apt-get install -y lvm2</code></pre></p>
<h3 id="_9">安装<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h3>
<p>我们这里部署最新的 release-1.2 版本的 Rook，部署清单文件地址：<a href="https://github.com/rook/rook/tree/release-1.2/cluster/examples/kubernetes/ceph">https://github.com/rook/rook/tree/release-1.2/cluster/examples/kubernetes/ceph</a>。</p>
<p>从上面链接中下载 common.yaml 与 operator.yaml 两个资源清单文件：
<pre class="highlight"><code class="language-shell"># 会安装crd、rbac相关资源对象
$ kubectl apply -f common.yaml
# 安装 rook operator
$ kubectl apply -f operator.yaml</code></pre></p>
<p>在继续操作之前，验证 <code>rook-ceph-operator</code> 是否处于<code>“Running”</code>状态：
<pre class="highlight"><code class="language-shell">$ kubectl get pods -n rook-ceph
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-operator-6d8fb9498b-jxdwx   1/1     Running   0          34s
rook-discover-7wpsl                   1/1     Running   0          32s
rook-discover-8t8lv                   1/1     Running   0          32s
rook-discover-9t497                   1/1     Running   0          32s
rook-discover-v57rd                   1/1     Running   0          32s</code></pre></p>
<p>我们可以看到 Operator 运行成功后，还会有一个 DaemonSet 控制器运行得 rook-discover 应用，当 <code>Rook Operator</code> 处于 Running 状态，我们就可以创建 Ceph 集群了。为了使集群在重启后不受影响，请确保设置的 <code>dataDirHostPath</code> 属性值为有效得主机路径。更多相关设置，可以查看<a href="https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html">集群配置相关文档</a>。</p>
<p>创建如下的资源清单文件：(cluster.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # 最新得 ceph 镜像, 可以查看 https://hub.docker.com/r/ceph/ceph/tags
    image: ceph/ceph:v14.2.5
  dataDirHostPath: /var/lib/rook  # 主机有效目录
  mon:
    count: 3
  dashboard:
    enabled: true
  storage:
    useAllNodes: true
    useAllDevices: false
    # 重要: Directories 应该只在预生产环境中使用
    directories:
    - path: /data/rook</code></pre></p>
<p>其中有几个比较重要的字段：</p>
<ul>
<li><code>dataDirHostPath</code>：宿主机上的目录，用于每个服务存储配置和数据。如果目录不存在，会自动创建该目录。由于此目录在主机上保留，因此在删除 Pod 后将保留该目录，另外不得使用以下路径及其任何子路径：<code>/etc/ceph</code>、<code>/rook</code> 或 <code>/var/log/ceph</code>。</li>
<li><code>useAllNodes</code>：用于表示是否使用集群中的所有节点进行存储，如果在 <code>nodes</code> 字段下指定了各个节点，则必须将<code>useAllNodes</code>设置为 false。</li>
<li><code>useAllDevices</code>：表示 OSD 是否自动使用节点上的所有设备，一般设置为 false，这样可控性较高</li>
<li><code>directories</code>：一般来说应该使用一块裸盘来做存储，有时为了测试方便，使用一个目录也是可以的，当然生成环境不推荐使用目录。</li>
</ul>
<p>除了上面这些字段属性之外还有很多其他可以细粒度控制得参数，可以查看<a href="https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html">集群配置相关文档</a>。</p>
<p>现在直接创建上面的 <code>CephCluster</code> 对象即可：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f cluster.yaml 
cephcluster.ceph.rook.io/rook-ceph created</code></pre></p>
<p>创建完成后，Rook Operator 就会根据我们的描述信息去自动创建 Ceph 集群了。</p>
<h3 id="_10">验证<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<p>要验证集群是否处于正常状态，我们可以使用 <a href="https://rook.io/docs/rook/v1.2/ceph-toolbox.html">Rook 工具箱</a> 来运行 <code>ceph status</code> 命令查看。</p>
<p>Rook 工具箱是一个用于调试和测试 Rook 的常用工具容器，该工具基于 CentOS 镜像，所以可以使用 yum 来轻松安装更多的工具包。我们这里用 Deployment 控制器来部署 Rook 工具箱，部署的资源清单文件如下所示：（toolbox.yaml）
<pre class="highlight"><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: rook-ceph-tools
  namespace: rook-ceph
  labels:
    app: rook-ceph-tools
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rook-ceph-tools
  template:
    metadata:
      labels:
        app: rook-ceph-tools
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: rook-ceph-tools
        image: rook/ceph:v1.2.1
        command: ["/tini"]
        args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
        imagePullPolicy: IfNotPresent
        env:
          - name: ROOK_ADMIN_SECRET
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: admin-secret
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /dev
            name: dev
          - mountPath: /sys/bus
            name: sysbus
          - mountPath: /lib/modules
            name: libmodules
          - name: mon-endpoint-volume
            mountPath: /etc/rook
      # 如果设置 hostNetwork: false,  "rbd map" 命令会被 hang 住, 参考 https://github.com/rook/rook/issues/2021
      hostNetwork: true
      volumes:
        - name: dev
          hostPath:
            path: /dev
        - name: sysbus
          hostPath:
            path: /sys/bus
        - name: libmodules
          hostPath:
            path: /lib/modules
        - name: mon-endpoint-volume
          configMap:
            name: rook-ceph-mon-endpoints
            items:
            - key: data
              path: mon-endpoints</code></pre></p>
<p>然后直接创建这个 Pod：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f toolbox.yaml
deployment.apps/rook-ceph-tools created</code></pre></p>
<p>一旦 toolbox 的 Pod 运行成功后，我们就可以使用下面的命令进入到工具箱内部进行操作：
<pre class="highlight"><code class="language-shell">$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash</code></pre></p>
<p>工具箱中的所有可用工具命令均已准备就绪，可满足您的故障排除需求。例如：
<pre class="highlight"><code class="language-shell">ceph status
ceph osd status
ceph df
rados df</code></pre></p>
<p>比如现在我们要查看集群的状态，需要满足下面的条件才认为是健康的：</p>
<ul>
<li>所有 mons 应该达到法定数量</li>
<li>mgr 应该是激活状态</li>
<li>至少有一个 OSD 处于激活状态</li>
<li>如果不是 HEALTH_OK 状态，则应该查看告警或者错误信息</li>
</ul>
<pre class="highlight"><code class="language-shell">$ ceph status
ceph status
  cluster:
    id:     dae083e6-8487-447b-b6ae-9eb321818439
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 15m)
    mgr: a(active, since 2m)
    osd: 31 osds: 2 up (since 6m), 2 in (since 6m)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   79 GiB used, 314 GiB / 393 GiB avail
    pgs:</code></pre>

<p>如果群集运行不正常，可以查看 <a href="https://rook.io/docs/rook/v1.2/ceph-common-issues.html">Ceph 常见问题</a>以了解更多详细信息和可能的解决方案。</p>
<h3 id="dashboard">Dashboard<a class="headerlink" href="#dashboard" title="Permanent link">&para;</a></h3>
<p>Ceph 有一个 Dashboard 工具，我们可以在上面查看集群的状态，包括总体运行状态，mgr、osd 和其他 Ceph 进程的状态，查看池和 PG 状态，以及显示守护进程的日志等等。</p>
<p>我们可以在上面的 cluster CRD 对象中开启 dashboard，设置<code>dashboard.enable=true</code>即可，这样 Rook Operator 就会启用 ceph-mgr dashboard 模块，并将创建一个 Kubernetes Service 来暴露该服务，将启用端口 7000 进行 https 访问，如果 Ceph 集群部署成功了，我们可以使用下面的命令来查看 Dashboard 的 Service：
<pre class="highlight"><code class="language-shell">$ kubectl get svc -n rook-ceph
NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr              ClusterIP   10.99.87.1       &lt;none&gt;        9283/TCP            3m6s
rook-ceph-mgr-dashboard    ClusterIP   10.111.195.180   &lt;none&gt;        7000/TCP            3m29s</code></pre></p>
<p>这里的 rook-ceph-mgr 服务用于报告 Prometheus metrics 指标数据的，而后面的的 rook-ceph-mgr-dashboard 服务就是我们的 Dashboard 服务，如果在集群内部我们可以通过 DNS 名称 <code>http://rook-ceph-mgr-dashboard.rook-ceph:7000</code>或者 CluterIP <code>http://10.111.195.180:7000</code> 来进行访问，但是如果要在集群外部进行访问的话，我们就需要通过 Ingress 或者 NodePort 类型的 Service 来暴露了，为了方便测试我们这里创建一个新的 NodePort 类型的服务来访问 Dashboard，资源清单如下所示：（dashboard-external.yaml）
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-mgr-dashboard-external
  namespace: rook-ceph
  labels:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
spec:
  ports:
  - name: dashboard
    port: 7000
    protocol: TCP
    targetPort: 7000
  selector:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
  type: NodePort</code></pre></p>
<p>同样直接创建即可：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f dashboard-external.yaml</code></pre></p>
<p>创建完成后我们可以查看到新创建的 rook-ceph-mgr-dashboard-external 这个 Service 服务：
<pre class="highlight"><code class="language-shell">$ kubectl get svc -n rook-ceph 
NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr                           ClusterIP   10.96.49.29     &lt;none&gt;        9283/TCP            23m
rook-ceph-mgr-dashboard                 ClusterIP   10.109.8.98     &lt;none&gt;        7000/TCP            23m
rook-ceph-mgr-dashboard-external   NodePort    10.109.53.223    &lt;none&gt;        7000:31361/TCP      14s</code></pre></p>
<p>现在我们需要通过 <code>http://&lt;NodeIp&gt;:31361</code> 就可以访问到 Dashboard 了。</p>
<p><img alt="ceph dashboard login" src="../../assets/img/storage/ceph-dashboard-login.png" /></p>
<p>但是在访问的时候需要我们登录才能够访问，Rook 创建了一个默认的用户 admin，并在运行 Rook 的命名空间中生成了一个名为 <code>rook-ceph-dashboard-admin-password</code> 的 Secret，要获取密码，可以运行以下命令：
<pre class="highlight"><code class="language-shell">$ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode &amp;&amp; echo
xxxx（登录密码）</code></pre></p>
<p>用上面获得的密码和用户名 admin 就可以登录 Dashboard 了，在 Dashboard 上面可以查看到整个集群的状态：</p>
<p><img alt="ceph dashboard" src="../../assets/img/storage/ceph-dashboard.jpg" /></p>
<h2 id="_11">使用<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h2>
<p>现在我们的 Ceph 集群搭建成功了，我们就可以来使用存储了。首先我们需要创建存储池，可以用 CRD 来定义 Pool。Rook 提供了两种机制来维持 OSD：</p>
<ul>
<li>副本：缺省选项，每个对象都会根据 <code>spec.replicated.size</code> 在多个磁盘上进行复制。建议非生产环境至少 2 个副本，生产环境至少 3 个。</li>
<li>Erasure Code：是一种较为节约的方式。EC 把数据拆分 n 段（<code>spec.erasureCoded.dataChunks</code>），再加入 k 个代码段（<code>spec.erasureCoded.codingChunks</code>），用分布的方式把 <code>n+k</code> 段数据保存在磁盘上。这种情况下 Ceph 能够隔离 k 个 OSD 的损失。</li>
</ul>
<p>我们这里使用副本的方式，创建如下所示的 RBD 类型的存储池：(pool.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: k8s-test-pool   # operator会监听并创建一个pool，执行完后界面上也能看到对应的pool
  namespace: rook-ceph
spec:
  failureDomain: host  # 数据块的故障域: 值为host时，每个数据块将放置在不同的主机上;值为osd时，每个数据块将放置在不同的osd上
  replicated:
    size: 3   # 池中数据的副本数,1就是不保存任何副本</code></pre></p>
<p>直接创建上面的资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f pool.yaml 
cephblockpool.ceph.rook.io/k8s-test-pool created</code></pre></p>
<p>存储池创建完成后我们在 Dashboard 上面的确可以看到新增了一个 pool，但是会发现集群健康状态变成了 <code>WARN</code>，我们可以查看到有如下日志出现：
<pre class="highlight"><code class="language-shell">Health check update: too few PGs per OSD (6 &lt; min 30) (TOO_FEW_PGS)</code></pre></p>
<p>这是因为每个 osd 上的 pg 数量小于最小的数目30个。pgs 为8，因为是3副本的配置，所以当有4个 osd 的时候，每个 osd 上均分了8/4 *3=6个pgs，也就是出现了如上的错误小于最小配置30个，集群这种状态如果进行数据的存储和操作，集群会卡死，无法响应io，同时会导致大面积的 osd down。</p>
<p>我们可以进入 toolbox 的容器中查看上面存储的 pg 数量：
<pre class="highlight"><code class="language-shell">$ ceph osd pool get k8s-test-pool pg_num
pg_num: 8</code></pre></p>
<p>我们可以通过增加 pg_num 来解决这个问题：
<pre class="highlight"><code class="language-shell">$ ceph osd pool set k8s-test-pool pg_num 64
set pool 1 pg_num to 64
$ ceph -s
  cluster:
    id:     7851387c-5d18-489a-8c04-b699fb9764c0
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 33m)
    mgr: a(active, since 32m)
    osd: 4 osds: 4 up (since 32m), 4 in (since 32m)

  data:
    pools:   1 pools, 64 pgs
    objects: 0 objects, 0 B
    usage:   182 GiB used, 605 GiB / 787 GiB avail
    pgs:     64 active+clean</code></pre></p>
<p>这个时候我们再查看就可以看到现在就是健康状态了。不过需要注意的是我们这里的 pool 上没有数据，所以修改 pg 影响并不大，但是如果是生产环境重新修改 pg 数，会对生产环境产生较大影响。因为 pg 数变了，就会导致整个集群的数据重新均衡和迁移，数据越大响应 io 的时间会越长。所以，最好在一开始就设置好 pg 数。</p>
<p>现在我们来创建一个 StorageClass 来进行动态存储配置，如下所示我们定义一个 Ceph 的块存储的 StorageClass：(storageclass.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  # clusterID 是 rook 集群运行的命名空间
  clusterID: rook-ceph

  # 指定存储池
  pool: k8s-test-pool

  # RBD image (实际的存储介质) 格式. 默认为 "2".
  imageFormat: "2"

  # RBD image 特性. CSI RBD 现在只支持 `layering` .
  imageFeatures: layering

  # Ceph 管理员认证信息，这些都是在 clusterID 命名空间下面自动生成的
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  # 指定 volume 的文件系统格式，如果不指定, csi-provisioner 会默认设置为 `ext4`
  csi.storage.k8s.io/fstype: ext4
# uncomment the following to use rbd-nbd as mounter on supported nodes
# **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi
# issue that causes the mount to be disconnected. You will need to follow special upgrade steps
# to restart your application pods. Therefore, this option is not recommended.
#mounter: rbd-nbd
reclaimPolicy: Delete</code></pre></p>
<p>直接创建上面的 StorageClass 资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f storageclass.yaml 
storageclass.storage.k8s.io/rook-ceph-block created
$ kubectl get storageclass
NAME              PROVISIONER                    AGE
rook-ceph-block   rook-ceph.rbd.csi.ceph.com     35s</code></pre></p>
<p>然后创建一个 PVC 来使用上面的 StorageClass 对象：(pvc.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  storageClassName: rook-ceph-block
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi</code></pre></p>
<p>同样直接创建上面的 PVC 资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f pvc.yaml 
persistentvolumeclaim/mysql-pv-claim created
$ kubectl get pvc -l app=wordpress
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Bound    pvc-1eab82e3-d214-4d8e-8fcc-ed379c24e0e3   20Gi       RWO            rook-ceph-block   32m</code></pre></p>
<p>创建完成后我们可以看到我们的 PVC 对象已经是 Bound 状态了，自动创建了对应的 PV，然后我们就可以直接使用这个 PVC 对象来做数据持久化操作了。</p>
<p>这个时候可能集群还会出现如下的健康提示：
<pre class="highlight"><code class="language-shell">$ ceph health detail
HEALTH_WARN application not enabled on 1 pool(s)
POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)
    application not enabled on pool 'k8s-test-pool'
    use 'ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;', where &lt;app-name&gt; is 'cephfs', 'rbd', 'rgw', or freeform for custom applications.
$ ceph osd pool application enable k8s-test-pool k8srbd
enabled application 'k8srbd' on pool 'k8s-test-pool'</code></pre></p>
<p>根据提示启用一个 application 即可。</p>
<p>在官方仓库 <a href="https://github.com/rook/rook/tree/release-1.2/cluster/examples/kubernetes">cluster/examples/kubernetes</a> 目录下，官方给了个 wordpress 的例子，可以直接运行测试即可：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f mysql.yaml
$ kubectl apply -f wordpress.yaml  </code></pre></p>
<p>官方的这个示例里面的 wordpress 用的 Loadbalancer 类型，我们可以改成 NodePort：
<pre class="highlight"><code class="language-shell">$ kubectl get pvc -l app=wordpress
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Bound    pvc-1eab82e3-d214-4d8e-8fcc-ed379c24e0e3   20Gi       RWO            rook-ceph-block   12h
wp-pv-claim      Bound    pvc-237932ed-5ca7-468c-bd16-220ebb2a1ce3   20Gi       RWO            rook-ceph-block   25s
$ kubectl get pods -l app=wordpress               
NAME                              READY   STATUS    RESTARTS   AGE
wordpress-5b886cf59b-4xwn8        1/1     Running   0          24m
wordpress-mysql-b9ddd6d4c-qhjd4   1/1     Running   0          24m
$ kubectl get svc -l app=wordpress
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
wordpress         NodePort    10.106.253.225   &lt;none&gt;        80:30307/TCP   80s
wordpress-mysql   ClusterIP   None             &lt;none&gt;        3306/TCP       87s</code></pre></p>
<p>当应用都处于 Running 状态后，我们可以通过 <code>http://&lt;任意节点IP&gt;:30307</code> 去访问 wordpress 应用：</p>
<p><img alt="ceph wordpress demo" src="../../assets/img/storage/ceph-wordpress-demo.png" /></p>
<p>比如我们在第一篇文章中更改下内容，然后我们将应用 Pod 全部删除重建：
<pre class="highlight"><code class="language-shell">$ kubectl delete pod wordpress-mysql-b9ddd6d4c-qhjd4 wordpress-5b886cf59b-4xwn8
pod "wordpress-mysql-b9ddd6d4c-qhjd4" deleted
pod "wordpress-5b886cf59b-4xwn8" deleted
$ kubectl get pods -l app=wordpress                                            
NAME                              READY   STATUS    RESTARTS   AGE
wordpress-5b886cf59b-kwxk4        1/1     Running   0          2m52s
wordpress-mysql-b9ddd6d4c-kkcr7   1/1     Running   0          2m52s</code></pre></p>
<p>当 Pod 重建完成后再次访问 wordpress 应用的主页我们可以发现之前我们添加的数据仍然存在，这就证明我们的数据持久化是正确的。</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      



<!-- Application footer -->
<footer class="md-footer">

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">

            <!-- Copyright and theme information -->
            <div class="md-footer-copyright">
                
                <div class="md-footer-copyright__highlight">
                    Copyright &copy; 2020 Kubernetes技术栈
                </div>
                
                powered by
                <a href="https://www.k8stech.net" title="Kubernetes技术栈">www.k8stech.net</a>
            </div>

            <!-- Social links -->
            
            
            
        </div>
    </div>
</footer>


    </div>
    
      <script src="../../assets/javascripts/application.d9aa80ab.js"></script>
      
        
        
          
          <script src="../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../assets/javascripts/lunr/lunr.jp.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="../../assets/js/hljs/highlight.pack.js"></script>
      
        <script src="../../assets/js/prism.js"></script>
      
    
  </body>
</html>