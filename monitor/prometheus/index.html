
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://docs.prometheus.cool/monitor/prometheus/">
      
      <link rel="icon" href="../../assets/img/prometheus_logo.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.4">
    
    
      
        <title>Prometheus - 云原生监控神器Prometheus</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.db9e7362.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ef5552">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Ubuntu";--md-code-font-family:"Ubuntu Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../assets/styles/extra.css">
    
      <link rel="stylesheet" href="../../assets/styles/prism.css">
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="red" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#prometheus" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="云原生监控神器Prometheus" class="md-header__button md-logo" aria-label="云原生监控神器Prometheus" data-md-component="logo">
      
  <img src="../../assets/img/prometheus_logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            云原生监控神器Prometheus
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Prometheus
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/k8stech/" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="云原生监控神器Prometheus" class="md-nav__button md-logo" aria-label="云原生监控神器Prometheus" data-md-component="logo">
      
  <img src="../../assets/img/prometheus_logo.png" alt="logo">

    </a>
    云原生监控神器Prometheus
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/k8stech/" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        介绍
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          监控基础与概述
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="监控基础与概述" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          监控基础与概述
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../monitorbasic/whymonitor/" class="md-nav__link">
        为什么要监控
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../monitorbasic/monitorterms/" class="md-nav__link">
        监控术语
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../monitorbasic/metricstory/" class="md-nav__link">
        指标物语
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../monitorbasic/monitorcontrast/" class="md-nav__link">
        开源监控对比
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Prometheus 基础
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus 基础" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Prometheus 基础
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../basic/prom-introduction/" class="md-nav__link">
        Prometheus介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../basic/tsdb-contrast/" class="md-nav__link">
        时序数据库对比
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../basic/prom-datamodel/" class="md-nav__link">
        数据模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../basic/node-exporter/" class="md-nav__link">
        Node_Exporter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../basic/prom-config/" class="md-nav__link">
        Prometheus安装与配置
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Prometheus 进阶
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus 进阶" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Prometheus 进阶
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/auto-ops-exporter-1/" class="md-nav__link">
        自动化维护Exporter（一）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/auto-ops-exporter-2/" class="md-nav__link">
        自动化维护Exporter（二）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/docker-swarm-monitor-1/" class="md-nav__link">
        Docker-Swarm集群监控（一）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/docker-swarm-monitor-2/" class="md-nav__link">
        Docker-Swarm集群监控（二）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced/commonly-exporter/" class="md-nav__link">
        常用Exporter介绍与配置
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Prometheus(警报)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus(警报)" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Prometheus(警报)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/promql-basic/" class="md-nav__link">
        PromQL详解（一）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/promql-operator/" class="md-nav__link">
        PromQL详解（二）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/promql-operator2/" class="md-nav__link">
        PromQL详解（三）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/promql-function/" class="md-nav__link">
        PromQL函数
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/alertmanager-overview/" class="md-nav__link">
        AlertManager
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/alertmanager-rules-1/" class="md-nav__link">
        Rules详解（一）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/alertmanager-rules-2/" class="md-nav__link">
        Rules详解（二）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/alertmanager-receiver/" class="md-nav__link">
        Receiver配置
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Alertmanager/alertmanager-silences/" class="md-nav__link">
        Silences配置
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Prometheus(联邦集群)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus(联邦集群)" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Prometheus(联邦集群)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../federation/federation-overview/" class="md-nav__link">
        Prometheus 联邦集群
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../federation/pushgateway/" class="md-nav__link">
        Pushgateway 代理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../federation/alertmanager-ha/" class="md-nav__link">
        Alertmanager 高可用
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Prometheus(服务发现)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus(服务发现)" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Prometheus(服务发现)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../discover-service/discovery-overview/" class="md-nav__link">
        服务发现（文件、DNS）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../discover-service/discovery-relabeling/" class="md-nav__link">
        服务发现（Relabelling）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../discover-service/discovery-consul/" class="md-nav__link">
        服务发现（Consul）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          Prometheus(Operator)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Prometheus(Operator)" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Prometheus(Operator)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Operator/" class="md-nav__link">
        Operator概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Statefulsets-1/" class="md-nav__link">
        手动部署（1）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Statefulsets-2/" class="md-nav__link">
        手动部署（2）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Statefulsets-3/" class="md-nav__link">
        手动部署（联邦）
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Deploy-Operator/" class="md-nav__link">
        Operator部署
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-HPA/" class="md-nav__link">
        HPA原理与实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Kubernetes/Prometheus-Adapter/" class="md-nav__link">
        Adapter原理与实践
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    简介
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    安装
  </a>
  
    <nav class="md-nav" aria-label="安装">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    示例应用
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    应用监控
  </a>
  
    <nav class="md-nav" aria-label="应用监控">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    普通应用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exporter" class="md-nav__link">
    使用 exporter 监控
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    集群节点
  </a>
  
    <nav class="md-nav" aria-label="集群节点">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    监控集群节点
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    服务发现
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    容器监控
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#apiserver" class="md-nav__link">
    监控 apiserver
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pod" class="md-nav__link">
    监控 Pod
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/k8stech/edit/master/docs/monitor/prometheus.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="prometheus">Prometheus<a class="headerlink" href="#prometheus" title="Permanent link">&para;</a></h1>
<p>我们知道监控是保证系统运行必不可少的功能，特别是对于 Kubernetes 这种比较庞大的系统来说，监控报警更是不可或缺，我们需要时刻了解系统的各种运行指标，也需要时刻了解我们的 Pod 的各种指标，更需要在出现问题的时候有报警信息通知到我们。</p>
<p>在早期的版本中 Kubernetes 提供了 heapster、influxDB、grafana 的组合来监控系统，在现在的版本中已经移除掉了 heapster，现在更加流行的监控工具是 <a href="https://prometheus.io/">Prometheus</a>，Prometheus 是 Google 内部监控报警系统的开源版本，是 Google SRE 思想在其内部不断完善的产物，它的存在是为了更快和高效的发现问题，快速的接入速度，简单灵活的配置都很好的解决了这一切，而且是已经毕业的 CNCF 项目。</p>
<h2 id="_1">简介<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>Prometheus 最初是 SoundCloud 构建的开源系统监控和报警工具，是一个独立的开源项目，于2016年加入了 CNCF 基金会，作为继 Kubernetes 之后的第二个托管项目。Prometheus 相比于其他传统监控工具主要有以下几个特点：</p>
<ul>
<li>具有由 metric 名称和键/值对标识的时间序列数据的多维数据模型</li>
<li>有一个灵活的查询语言</li>
<li>不依赖分布式存储，只和本地磁盘有关</li>
<li>通过 HTTP 的服务拉取时间序列数据</li>
<li>也支持推送的方式来添加时间序列数据</li>
<li>还支持通过服务发现或静态配置发现目标</li>
<li>多种图形和仪表板支持</li>
</ul>
<p>Prometheus 由多个组件组成，但是其中有些组件是可选的：</p>
<ul>
<li><code>Prometheus Server</code>：用于抓取指标、存储时间序列数据</li>
<li><code>exporter</code>：暴露指标让任务来抓</li>
<li><code>pushgateway</code>：push 的方式将指标数据推送到该网关</li>
<li><code>alertmanager</code>：处理报警的报警组件
<code>adhoc</code>：用于数据查询</li>
</ul>
<p>大多数 Prometheus 组件都是用 Go 编写的，因此很容易构建和部署为静态的二进制文件。下图是 Prometheus 官方提供的架构及其一些相关的生态系统组件：</p>
<p><img alt="prometheus architecture" src="../../assets/img/monitor/prometheus-architecture.png" /></p>
<p>整体流程比较简单，Prometheus 直接接收或者通过中间的 Pushgateway 网关被动获取指标数据，在本地存储所有的获取的指标数据，并对这些数据进行一些规则整理，用来生成一些聚合数据或者报警信息，Grafana 或者其他工具用来可视化这些数据。</p>
<h2 id="_2">安装<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>由于 Prometheus 是 Golang 编写的程序，所以要安装的话也非常简单，只需要将二进制文件下载下来直接执行即可，前往地址：<a href="https://prometheus.io/download">https://prometheus.io/download</a> 下载最新版本即可。</p>
<p>Prometheus 是通过一个 YAML 配置文件来进行启动的，如果我们使用二进制的方式来启动的话，可以使用下面的命令：
<pre class="highlight"><code class="language-shell">$ ./prometheus --config.file=prometheus.yml</code></pre></p>
<p>其中 <code>prometheus.yml</code> 文件的基本配置如下：
<pre class="highlight"><code class="language-yaml">global:
  scrape_interval:     15s
  evaluation_interval: 15s

rule_files:
  # - "first.rules"
  # - "second.rules"

scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets: ['localhost:9090']</code></pre></p>
<p>上面这个配置文件中包含了3个模块：<code>global</code>、<code>rule_files</code> 和 <code>scrape_configs</code>。</p>
<ul>
<li>
<p><code>global</code> 模块控制 <code>Prometheus Server</code> 的全局配置：</p>
<ul>
<li><code>scrape_interval</code>：表示 prometheus 抓取指标数据的频率，默认是15s，我们可以覆盖这个值</li>
<li><code>evaluation_interval</code>：用来控制评估规则的频率，prometheus 使用规则产生新的时间序列数据或者产生警报</li>
</ul>
</li>
<li>
<p><code>rule_files</code>：指定了报警规则所在的位置，prometheus 可以根据这个配置加载规则，用于生成新的时间序列数据或者报警信息，当前我们没有配置任何报警规则。</p>
</li>
<li>
<p><code>scrape_configs</code> 用于控制 prometheus 监控哪些资源。</p>
</li>
</ul>
<p>由于 prometheus 通过 HTTP 的方式来暴露的它本身的监控数据，prometheus 也能够监控本身的健康情况。在默认的配置里有一个单独的 job，叫做 prometheus，它采集 prometheus 服务本身的时间序列数据。这个 job 包含了一个单独的、静态配置的目标：监听 localhost 上的 9090 端口。prometheus 默认会通过目标的 <code>/metrics</code> 路径采集 metrics。所以，默认的 job 通过 URL：<code>http://localhost:9090/metrics</code> 采集 metrics。收集到的时间序列包含 prometheus 服务本身的状态和性能。如果我们还有其他的资源需要监控的话，直接配置在 <code>scrape_configs</code> 模块下面就可以了。</p>
<h3 id="_3">示例应用<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>比如我们在本地启动一些样例来让 Prometheus 采集。Go 客户端库包含一个示例，该示例为具有不同延迟分布的三个服务暴露 RPC 延迟。</p>
<p>首先确保已经安装了 Go 环境并启用 go modules，下载 Prometheus 的 Go 客户端库并运行这三个示例：
<pre class="highlight"><code class="language-shell">$ git clone https://github.com/prometheus/client_golang.git
$ cd client_golang/examples/random
$ export GO111MODULE=on   
$ export GOPROXY=https://goproxy.cn
$ go build</code></pre></p>
<p>然后在3个独立的终端里面运行3个服务：
<pre class="highlight"><code class="language-shell">$ ./random -listen-address=:8080
$ ./random -listen-address=:8081
$ ./random -listen-address=:8082</code></pre></p>
<p>这个时候我们可以得到3个不同的监控接口：http://localhost:8080/metrics、http://localhost:8081/metrics 和 http://localhost:8082/metrics。</p>
<p>现在我们配置 Prometheus 来采集这些新的目标，让我们将这三个目标分组到一个名为 example-random 的任务。假设前两个端点（即：http://localhost:8080/metrics、http://localhost:8081/metrics ）都是生产级目标应用，第三个端点（即：http://localhost:8082/metrics ）为金丝雀实例。要在 Prometheus 中对此进行建模，我们可以将多组端点添加到单个任务中，为每组目标添加额外的标签。 在此示例中，我们将 <code>group =“production”</code> 标签添加到第一组目标，同时将 <code>group=“canary”</code>添加到第二组。将以下配置添加到 <code>prometheus.yml</code> 中的 <code>scrape_configs</code> 部分，然后重新启动 Prometheus 实例：
<pre class="highlight"><code class="language-yaml">scrape_configs:
  - job_name: 'example-random'
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:8080', 'localhost:8081']
        labels:
          group: 'production'
      - targets: ['localhost:8082']
        labels:
          group: 'canary'</code></pre></p>
<p>然后我们可以到浏览器中查看 Prometheus 的配置是否有新增的任务，这就是 Prometheus 添加监控配置最基本的配置方式了，非常简单，只需要提供一个符合 metrics 格式的可访问的接口配置给 Prometheus 就可以了。</p>
<p>但是由于我们这里是要运行在 Kubernetes 系统中，所以我们直接用 Docker 镜像的方式运行。</p>
<div class="admonition info">
<p class="admonition-title">命名空间</p>
<p>为了方便管理，我们将监控相关的所有资源对象都安装在 <code>kube-mon</code> 这个 namespace 下面，没有的话可以提前创建。</p>
</div>
<p>为了能够方便的管理配置文件，我们这里将 <code>prometheus.yml</code> 文件用 ConfigMap 的形式进行管理：（prometheus-cm.yaml）
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-mon
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']</code></pre></p>
<p>我们这里暂时只配置了对 prometheus 本身的监控，直接创建该资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap "prometheus-config" created</code></pre></p>
<p>配置文件创建完成了，以后如果我们有新的资源需要被监控，我们只需要将上面的 ConfigMap 对象更新即可。现在我们来创建 prometheus 的 Pod 资源：(prometheus-deploy.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: kube-mon
  labels:
    app: prometheus
spec:
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      nodeSelector:
        monitor: prometheus
      containers:
      - image: prom/prometheus:v2.14.0
        name: prometheus
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--storage.tsdb.path=/prometheus"  # 指定tsdb数据路径
        - "--storage.tsdb.retention.time=24h"
        - "--web.enable-admin-api"  # 控制对admin HTTP API的访问，其中包括删除时间序列等功能
        - "--web.enable-lifecycle"  # 支持热更新，直接执行localhost:9090/-/reload立即生效
        - "--web.console.libraries=/usr/share/prometheus/console_libraries"
        - "--web.console.templates=/usr/share/prometheus/consoles"
        ports:
        - containerPort: 9090
          name: http
        volumeMounts:
        - mountPath: "/etc/prometheus"
          name: config-volume
        - mountPath: "/prometheus"
          name: data
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 100m
            memory: 512Mi
      volumes:
      - name: data
        hostPath:
          path: /data/prometheus/
      - configMap:
          name: prometheus-config
        name: config-volume</code></pre></p>
<p>另外为了 prometheus 的性能和数据持久化我们这里是直接将通过 hostPath 的方式来进行数据持久化的，通过 <code>--storage.tsdb.path=/prometheus</code> 指定数据目录，然后将该目录声明挂载到 <code>/data/prometheus</code> 这个主机目录下面，为了防止 Pod 漂移，所以我们使用 <code>nodeSelector</code> 将 Pod 固定到了一个具有 <code>monitor=prometheus</code> 标签的节点上，如果没有这个标签则需要为你的目标节点打上这个标签：
<pre class="highlight"><code class="language-shell">$ kubectl label node ydzs-node3 monitor=prometheus
node/ydzs-node3 labeled</code></pre></p>
<p>由于 prometheus 可以访问 Kubernetes 的一些资源对象，所以需要配置 rbac 相关认证，这里我们使用了一个名为 prometheus 的 serviceAccount 对象：(prometheus-rbac.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: kube-mon
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - "extensions"
  resources:
    - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: kube-mon</code></pre></p>
<p>由于我们要获取的资源信息，在每一个 namespace 下面都有可能存在，所以我们这里使用的是 <code>ClusterRole</code> 的资源对象，值得一提的是我们这里的权限规则声明中有一个 <code>nonResourceURLs</code> 的属性，是用来对非资源型 metrics 进行操作的权限声明，这个在以前我们很少遇到过，然后直接创建上面的资源对象即可：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-rbac.yaml
serviceaccount "prometheus" created
clusterrole.rbac.authorization.k8s.io "prometheus" created
clusterrolebinding.rbac.authorization.k8s.io "prometheus" created</code></pre></p>
<p>现在我们就可以添加 promethues 的资源对象了：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-deploy.yaml 
deployment.apps/prometheus created
$ kubectl get pods -n kube-mon
NAME                         READY   STATUS             RESTARTS   AGE
prometheus-df4f47d95-vksmc   0/1     CrashLoopBackOff   3          98s
$ kubectl logs -f prometheus-df4f47d95-vksmc -n kube-mon
level=info ts=2019-12-12T03:08:49.424Z caller=main.go:332 msg="Starting Prometheus" version="(version=2.14.0, branch=HEAD, revision=edeb7a44cbf745f1d8be4ea6f215e79e651bfe19)"
level=info ts=2019-12-12T03:08:49.424Z caller=main.go:333 build_context="(go=go1.13.4, user=root@df2327081015, date=20191111-14:27:12)"
level=info ts=2019-12-12T03:08:49.425Z caller=main.go:334 host_details="(Linux 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 prometheus-df4f47d95-vksmc (none))"
level=info ts=2019-12-12T03:08:49.425Z caller=main.go:335 fd_limits="(soft=1048576, hard=1048576)"
level=info ts=2019-12-12T03:08:49.425Z caller=main.go:336 vm_limits="(soft=unlimited, hard=unlimited)"
level=error ts=2019-12-12T03:08:49.425Z caller=query_logger.go:85 component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker(0x7ffd8cf6ec5d, 0xb, 0x14, 0x2b4f400, 0xc0006f33b0, 0x2b4f400)
        /app/promql/query_logger.go:115 +0x48c
main.main()
        /app/cmd/prometheus/main.go:364 +0x5229</code></pre></p>
<p>创建 Pod 后，我们可以看到并没有成功运行，出现了 <code>open /prometheus/queries.active: permission denied</code> 这样的错误信息，这是因为我们的 prometheus 的镜像中是使用的 nobody 这个用户，然后现在我们通过 hostPath 挂载到宿主机上面的目录的 <code>ownership</code> 却是 <code>root</code>：
<pre class="highlight"><code class="language-shell">$ ls -la /data/
total 36
drwxr-xr-x   6 root root  4096 Dec 12 11:07 .
dr-xr-xr-x. 19 root root  4096 Nov  9 23:19 ..
drwxr-xr-x   2 root root  4096 Dec 12 11:07 prometheus</code></pre></p>
<p>所以当然会出现操作权限问题了，这个时候我们就可以通过 <code>securityContext</code> 来为 Pod 设置下 volumes 的权限，通过设置 <code>runAsUser=0</code> 指定运行的用户为 root：
<pre class="highlight"><code class="language-yaml">......
securityContext:
  runAsUser: 0
volumes:
- name: data
  hostPath:
    path: /data/prometheus/
- configMap:
    name: prometheus-config
  name: config-volume</code></pre></p>
<p>这个时候我们重新更新下 prometheus：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-deploy.yaml
deployment.apps/prometheus configured
$ kubectl get pods -n kube-mon                              
NAME                          READY   STATUS    RESTARTS   AGE
prometheus-79b8774f68-7m8zr   1/1     Running   0          56s
$ kubectl logs -f prometheus-79b8774f68-7m8zr -n kube-mon
level=info ts=2019-12-12T03:17:44.228Z caller=main.go:332 msg="Starting Prometheus" version="(version=2.14.0, branch=HEAD, revision=edeb7a44cbf745f1d8be4ea6f215e79e651bfe19)"
......
level=info ts=2019-12-12T03:17:44.822Z caller=main.go:673 msg="TSDB started"
level=info ts=2019-12-12T03:17:44.822Z caller=main.go:743 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2019-12-12T03:17:44.827Z caller=main.go:771 msg="Completed loading of configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2019-12-12T03:17:44.827Z caller=main.go:626 msg="Server is ready to receive web requests."</code></pre></p>
<p>Pod 创建成功后，为了能够在外部访问到 prometheus 的 webui 服务，我们还需要创建一个 Service 对象：(prometheus-svc.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: kube-mon
  labels:
    app: prometheus
spec:
  selector:
    app: prometheus
  type: NodePort
  ports:
    - name: web
      port: 9090
      targetPort: http</code></pre></p>
<p>为了方便测试，我们这里创建一个 <code>NodePort</code> 类型的服务，当然我们可以创建一个 <code>Ingress</code>对象，通过域名来进行访问：
<pre class="highlight"><code class="language-shell">$ $ kubectl apply -f prometheus-svc.yaml
service "prometheus" created
$ kubectl get svc -n kube-mon
NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
prometheus   NodePort   10.96.194.29   &lt;none&gt;        9090:30980/TCP   13h</code></pre></p>
<p>现在我们就可以通过 <code>http://任意节点IP:30980</code> 访问 prometheus 的 webui 服务了：</p>
<p><img alt="prometheus webui" src="../../assets/img/monitor/prometheus-webui.png" /></p>
<p>现在我们可以查看当前监控系统中的一些监控目标（Status -&gt; Targets）：</p>
<p><img alt="prometheus webui targets" src="../../assets/img/monitor/prometheus-webui-targets.png" /></p>
<p>由于我们现在还没有配置任何的报警信息，所以 <code>Alerts</code> 菜单下面现在没有任何数据，隔一会儿，我们可以去 <code>Graph</code> 菜单下面查看我们抓取的 prometheus 本身的一些监控数据了，其中<code>- insert metrics at cursor -</code>下面就有我们搜集到的一些监控指标数据：</p>
<p><img alt="prometheus webui metrics" src="../../assets/img/monitor/prometheus-webui-metrics.png" /></p>
<p>比如我们这里就选择 <code>scrape_duration_seconds</code> 这个指标，然后点击 <code>Execute</code>，就可以看到类似于下面的图表数据了：</p>
<p><img alt="prometheus webui query" src="../../assets/img/monitor/prometheus-webui-query.png" /></p>
<p>除了简单的直接使用采集到的一些监控指标数据之外，这个时候也可以使用强大的 <code>PromQL</code> 工具，<code>PromQL</code> 其实就是 prometheus 便于数据聚合展示开发的一套 <code>ad hoc</code> 查询语言的，你想要查什么找对应函数取你的数据好了。</p>
<h2 id="_4">应用监控<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>前面我们和大家介绍了 Prometheus 的数据指标是通过一个公开的 HTTP(S) 数据接口获取到的，我们不需要单独安装监控的 agent，只需要暴露一个 metrics 接口，Prometheus 就会定期去拉取数据；对于一些普通的 HTTP 服务，我们完全可以直接重用这个服务，添加一个 <code>/metrics</code> 接口暴露给 Prometheus；而且获取到的指标数据格式是非常易懂的，不需要太高的学习成本。</p>
<p>现在很多服务从一开始就内置了一个 <code>/metrics</code> 接口，比如 Kubernetes 的各个组件、istio 服务网格都直接提供了数据指标接口。有一些服务即使没有原生集成该接口，也完全可以使用一些 <code>exporter</code> 来获取到指标数据，比如 <code>mysqld_exporter</code>、<code>node_exporter</code>，这些 <code>exporter</code> 就有点类似于传统监控服务中的 agent，作为服务一直存在，用来收集目标服务的指标数据然后直接暴露给 Prometheus。</p>
<h3 id="_5">普通应用<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>对于普通应用只需要能够提供一个满足 prometheus 格式要求的 <code>/metrics</code> 接口就可以让 Prometheus 来接管监控，比如 Kubernetes 集群中非常重要的 CoreDNS 插件，一般默认情况下就开启了 <code>/metrics</code> 接口：
<pre class="highlight"><code class="language-shell">$ kubectl get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
kind: ConfigMap
metadata:
  creationTimestamp: "2019-11-08T11:59:49Z"
  name: coredns
  namespace: kube-system
  resourceVersion: "188"
  selfLink: /api/v1/namespaces/kube-system/configmaps/coredns
  uid: 21966186-c2d9-467a-b87f-d061c5c9e4d7</code></pre></p>
<p>上面 ConfigMap 中 <code>prometheus :9153</code> 就是开启 prometheus 的插件：
<pre class="highlight"><code class="language-shell">$ kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
NAME                       READY   STATUS    RESTARTS   AGE     IP            NODE         NOMINATED NODE   READINESS GATES
coredns-667f964f9b-sthqq   1/1     Running   0          4d20h   10.244.1.15   ydzs-node1   &lt;none&gt;           &lt;none&gt;
coredns-667f964f9b-zj4r4   1/1     Running   0          4d20h   10.244.2.127   ydzs-node3   &lt;none&gt;           &lt;none&gt;</code></pre></p>
<p>我们可以先尝试手动访问下 <code>/metrics</code> 接口，如果能够手动访问到那证明接口是没有任何问题的：
<pre class="highlight"><code class="language-shell">$ curl http://10.244.1.15:9153/metrics
# HELP coredns_build_info A metric with a constant '1' value labeled by version, revision, and goversion from which CoreDNS was built.
# TYPE coredns_build_info gauge
coredns_build_info{goversion="go1.12.8",revision="795a3eb",version="1.6.2"} 1
# HELP coredns_cache_hits_total The count of cache hits.
# TYPE coredns_cache_hits_total counter
coredns_cache_hits_total{server="dns://:53",type="success"} 4
# HELP coredns_cache_misses_total The count of cache misses.
# TYPE coredns_cache_misses_total counter
coredns_cache_misses_total{server="dns://:53"} 15
# HELP coredns_cache_size The number of elements in the cache.
# TYPE coredns_cache_size gauge
coredns_cache_size{server="dns://:53",type="denial"} 5
coredns_cache_size{server="dns://:53",type="success"} 4
......</code></pre></p>
<p>我们可以看到可以正常访问到，从这里可以看到 CoreDNS 的监控数据接口是正常的了，然后我们就可以将这个 <code>/metrics</code> 接口配置到 <code>prometheus.yml</code> 中去了，直接加到默认的 prometheus 这个 <code>job</code> 下面：(prome-cm.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: kube-mon
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s

    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
        - targets: ['localhost:9090']

    - job_name: 'coredns'
      static_configs:
        - targets: ['10.244.1.15:9153', '10.244.2.127:9153']</code></pre></p>
<p>当然，我们这里只是一个很简单的配置，<code>scrape_configs</code> 下面可以支持很多参数，例如：</p>
<ul>
<li><code>basic_auth</code> 和 <code>bearer_token</code>：比如我们提供的 <code>/metrics</code> 接口需要 basic 认证的时候，通过传统的用户名/密码或者在请求的 header 中添加对应的 token 都可以支持</li>
<li><code>kubernetes_sd_configs</code> 或 <code>consul_sd_configs</code>：可以用来自动发现一些应用的监控数据</li>
</ul>
<p>现在我们重新更新这个 ConfigMap 资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured</code></pre></p>
<p>现在 Prometheus 的配置文件内容已经更改了，隔一会儿被挂载到 Pod 中的 prometheus.yml 文件也会更新，由于我们之前的 Prometheus 启动参数中添加了 <code>--web.enable-lifecycle</code> 参数，所以现在我们只需要执行一个 <code>reload</code> 命令即可让配置生效：
<pre class="highlight"><code class="language-shell">$ kubectl get pods -n kube-mon -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
prometheus-79b8774f68-7m8zr   1/1     Running   0          28m   10.244.3.174   ydzs-node3   &lt;none&gt;           &lt;none&gt;
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre></p>
<div class="admonition info">
<p class="admonition-title">热更新</p>
<p>由于 ConfigMap 通过 Volume 的形式挂载到 Pod 中去的热更新需要一定的间隔时间才会生效，所以需要稍微等一小会儿。</p>
</div>
<p>这个时候我们再去看 Prometheus 的 Dashboard 中查看采集的目标数据：</p>
<p><img alt="prometheus webui coredns" src="../../assets/img/monitor/prometheus-webui-coredns.png" /></p>
<p>可以看到我们刚刚添加的 coredns 这个任务已经出现了，然后同样的我们可以切换到 Graph 下面去，我们可以找到一些 CoreDNS 的指标数据，至于这些指标数据代表什么意义，一般情况下，我们可以去查看对应的 <code>/metrics</code> 接口，里面一般情况下都会有对应的注释。</p>
<p><img alt="prometheus webui coredns metrics" src="../../assets/img/monitor/prometheus-webui-coredns-metrics.png" /></p>
<p>到这里我们就在 Prometheus 上配置了第一个 Kubernetes 应用。</p>
<h3 id="exporter">使用 exporter 监控<a class="headerlink" href="#exporter" title="Permanent link">&para;</a></h3>
<p>上面我们也说过有一些应用可能没有自带 <code>/metrics</code> 接口供 Prometheus 使用，在这种情况下，我们就需要利用 <code>exporter</code> 服务来为 Prometheus 提供指标数据了。Prometheus 官方为许多应用就提供了对应的 <code>exporter</code> 应用，也有许多第三方的实现，我们可以前往官方网站进行查看：<a href="https://prometheus.io/docs/instrumenting/exporters/">exporters</a>，当然如果你的应用本身也没有 exporter 实现，那么就要我们自己想办法去实现一个 <code>/metrics</code> 接口了，只要你能提供一个合法的 <code>/metrics</code> 接口，Prometheus 就可以监控你的应用。</p>
<p>比如我们这里通过一个 <a href="https://github.com/oliver006/redis_exporter">redis-exporter</a> 的服务来监控 redis 服务，对于这类应用，我们一般会以 <code>sidecar</code> 的形式和主应用部署在同一个 Pod 中，比如我们这里来部署一个 redis 应用，并用 redis-exporter 的方式来采集监控数据供 Prometheus 使用，如下资源清单文件：（prome-redis.yaml）
<pre class="highlight"><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: kube-mon
spec:
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9121"
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:4
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
      - name: redis-exporter
        image: oliver006/redis_exporter:latest
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 9121
---
kind: Service
apiVersion: v1
metadata:
  name: redis
  namespace: kube-mon
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
  - name: prom
    port: 9121
    targetPort: 9121</code></pre></p>
<p>可以看到上面我们在 redis 这个 Pod 中包含了两个容器，一个就是 redis 本身的主应用，另外一个容器就是 redis_exporter。现在直接创建上面的应用：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prome-redis.yaml
deployment.apps/redis created
service/redis created</code></pre></p>
<p>创建完成后，我们可以看到 redis 的 Pod 里面包含有两个容器：
<pre class="highlight"><code class="language-shell">$ kubectl get pods -n kube-mon
NAME                          READY   STATUS    RESTARTS   AGE
prometheus-79b8774f68-7m8zr   1/1     Running   0          54m
redis-7c8bdd45cc-ssjbz        2/2     Running   0          2m1s
$ kubectl get svc -n kube-mon
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
prometheus   NodePort    10.96.194.29   &lt;none&gt;        9090:30980/TCP      15h
redis        ClusterIP   10.110.14.69   &lt;none&gt;        6379/TCP,9121/TCP   2m14s</code></pre></p>
<p>我们可以通过 9121 端口来校验是否能够采集到数据：
<pre class="highlight"><code class="language-shell">$ curl 10.110.14.69:9121/metrics
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
......
# HELP redis_up Information about the Redis instance
# TYPE redis_up gauge
redis_up 1
# HELP redis_uptime_in_seconds uptime_in_seconds metric
# TYPE redis_uptime_in_seconds gauge
redis_uptime_in_seconds 100</code></pre></p>
<p>同样的，现在我们只需要更新 Prometheus 的配置文件：
<pre class="highlight"><code class="language-yaml">- job_name: 'redis'
  static_configs:
  - targets: ['redis:9121']</code></pre></p>
<p>由于我们这里是通过 Service 去配置的 redis 服务，当然直接配置 Pod IP 也是可以的，因为和 Prometheus 处于同一个 namespace，所以我们直接使用 servicename 即可。配置文件更新后，重新加载：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre></p>
<p>这个时候我们再去看 Prometheus 的 Dashboard 中查看采集的目标数据： </p>
<p><img alt="prometheus webui redis" src="../../assets/img/monitor/prometheus-webui-redis.png" /></p>
<p>可以看到配置的 redis 这个 job 已经生效了。切换到 Graph 下面可以看到很多关于 redis 的指标数据，我们选择任意一个指标，比如 <code>redis_exporter_scrapes_total</code>，然后点击执行就可以看到对应的数据图表了：</p>
<p><img alt="prometheus webui redisquery" src="../../assets/img/monitor/prometheus-webui-redis-query.png" /></p>
<h2 id="_6">集群节点<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<p>前面我们和大家学习了怎样用 Promethues 来监控 Kubernetes 集群中的应用，但是对于 Kubernetes 集群本身的监控也是非常重要的，我们需要时时刻刻了解集群的运行状态。</p>
<p>对于集群的监控一般我们需要考虑以下几个方面：</p>
<ul>
<li>Kubernetes 节点的监控：比如节点的 cpu、load、disk、memory 等指标</li>
<li>内部系统组件的状态：比如 kube-scheduler、kube-controller-manager、kubedns/coredns 等组件的详细运行状态</li>
<li>编排级的 metrics：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标</li>
</ul>
<p>Kubernetes 集群的监控方案目前主要有以下几种方案：</p>
<ul>
<li>Heapster：Heapster 是一个集群范围的监控和数据聚合工具，以 Pod 的形式运行在集群中。 heapster 除了 Kubelet/cAdvisor 之外，我们还可以向 Heapster 添加其他指标源数据，比如 kube-state-metrics，需要注意的是 Heapster 已经被废弃了，后续版本中会使用 metrics-server 代替。</li>
<li>cAdvisor：<a href="https://github.com/google/cadvisor">cAdvisor</a> 是 Google 开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器。</li>
<li>kube-state-metrics：<a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a> 通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。</li>
<li>metrics-server：metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。</li>
</ul>
<p>不过 kube-state-metrics 和 metrics-server 之间还是有很大不同的，二者的主要区别如下：</p>
<ul>
<li>kube-state-metrics 主要关注的是业务相关的一些元数据，比如 Deployment、Pod、副本状态等</li>
<li>metrics-server 主要关注的是<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md">资源度量 API</a> 的实现，比如 CPU、文件描述符、内存、请求延时等指标。</li>
</ul>
<h3 id="_7">监控集群节点<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<p>要监控节点其实我们已经有很多非常成熟的方案了，比如 Nagios、zabbix，甚至我们自己来收集数据也可以，我们这里通过 Prometheus 来采集节点的监控指标数据，可以通过 <a href="https://github.com/prometheus/node_exporter">node_exporter</a> 来获取，顾名思义，<code>node_exporter</code> 就是抓取用于采集服务器节点的各种运行指标，目前 <code>node_exporter</code> 支持几乎所有常见的监控点，比如 conntrack，cpu，diskstats，filesystem，loadavg，meminfo，netstat 等，详细的监控点列表可以参考其 <a href="https://github.com/prometheus/node_exporter">Github 仓库</a>。</p>
<p>我们可以通过 DaemonSet 控制器来部署该服务，这样每一个节点都会自动运行一个这样的 Pod，如果我们从集群中删除或者添加节点后，也会进行自动扩展。</p>
<p>在部署 <code>node-exporter</code> 的时候有一些细节需要注意，如下资源清单文件：(prome-node-exporter.yaml)
<pre class="highlight"><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: kube-mon
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostPID: true
      hostIPC: true
      hostNetwork: true
      nodeSelector:
        kubernetes.io/os: linux
      containers:
      - name: node-exporter
        image: prom/node-exporter:v0.18.1
        args:
        - --web.listen-address=$(HOSTIP):9100
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
        ports:
        - containerPort: 9100
        env:
        - name: HOSTIP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        resources:
          requests:
            cpu: 150m
            memory: 180Mi
          limits:
            cpu: 150m
            memory: 180Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        volumeMounts:
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: root
          mountPath: /host/root
          mountPropagation: HostToContainer
          readOnly: true
      tolerations:
      - operator: "Exists"
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: dev
        hostPath:
          path: /dev
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /</code></pre></p>
<p>由于我们要获取到的数据是主机的监控指标数据，而我们的 <code>node-exporter</code> 是运行在容器中的，所以我们在 Pod 中需要配置一些 Pod 的安全策略，这里我们就添加了 <code>hostPID: true</code>、<code>hostIPC: true</code>、<code>hostNetwork: true</code> 3个策略，用来使用主机的 <code>PID namespace</code>、<code>IPC namespace</code> 以及主机网络，这些 namespace 就是用于容器隔离的关键技术，要注意这里的 namespace 和集群中的 namespace 是两个完全不相同的概念。</p>
<p>另外我们还将主机的 <code>/dev</code>、<code>/proc</code>、<code>/sys</code>这些目录挂载到容器中，这些因为我们采集的很多节点数据都是通过这些文件夹下面的文件来获取到的，比如我们在使用 <code>top</code> 命令可以查看当前 cpu 使用情况，数据就来源于文件 <code>/proc/stat</code>，使用 <code>free</code> 命令可以查看当前内存使用情况，其数据来源是来自 <code>/proc/meminfo</code> 文件。</p>
<p>另外由于我们集群使用的是 <code>kubeadm</code> 搭建的，所以如果希望 master 节点也一起被监控，则需要添加相应的容忍，然后直接创建上面的资源对象：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prome-node-exporter.yaml
daemonset.apps/node-exporter created
$ kubectl get pods -n kube-mon -l app=node-exporter -o wide
NAME                  READY   STATUS    RESTARTS   AGE    IP             NODE          NOMINATED NODE   READINESS GATES
node-exporter-cd2cq   1/1     Running   0          107s   10.151.30.57   ydzs-node3    &lt;none&gt;           &lt;none&gt;
node-exporter-l6jv6   1/1     Running   0          107s   10.151.30.23   ydzs-node2    &lt;none&gt;           &lt;none&gt;
node-exporter-qv4x5   1/1     Running   0          107s   10.151.30.59   ydzs-node4    &lt;none&gt;           &lt;none&gt;
node-exporter-vbbhc   1/1     Running   0          107s   10.151.30.11   ydzs-master   &lt;none&gt;           &lt;none&gt;
node-exporter-wlgnz   1/1     Running   0          107s   10.151.30.22   ydzs-node1    &lt;none&gt;           &lt;none&gt;</code></pre></p>
<p>部署完成后，我们可以看到在5个节点上都运行了一个 Pod，由于我们指定了 <code>hostNetwork=true</code>，所以在每个节点上就会绑定一个端口 9100，我们可以通过这个端口去获取到监控指标数据：
<pre class="highlight"><code class="language-shell">$ curl 10.151.30.11:9100/metrics
...
node_filesystem_device_error{device="shm",fstype="tmpfs",mountpoint="/rootfs/var/lib/docker/containers/aefe8b1b63c3aa5f27766053ec817415faf8f6f417bb210d266fef0c2da64674/shm"} 1
node_filesystem_device_error{device="shm",fstype="tmpfs",mountpoint="/rootfs/var/lib/docker/containers/c8652ca72230496038a07e4fe4ee47046abb5f88d9d2440f0c8a923d5f3e133c/shm"} 1
node_filesystem_device_error{device="tmpfs",fstype="tmpfs",mountpoint="/dev"} 0
node_filesystem_device_error{device="tmpfs",fstype="tmpfs",mountpoint="/dev/shm"} 0
...</code></pre></p>
<p>当然如果你觉得上面的手动安装方式比较麻烦，我们也可以使用 Helm 的方式来安装：
<pre class="highlight"><code class="language-shell">$ helm upgrade --install node-exporter --namespace kube-mon stable/prometheus-node-exporter</code></pre></p>
<h3 id="_8">服务发现<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<p>由于我们这里每个节点上面都运行了 <code>node-exporter</code> 程序，如果我们通过一个 Service 来将数据收集到一起用静态配置的方式配置到 Prometheus 去中，就只会显示一条数据，我们得自己在指标数据中去过滤每个节点的数据，当然我们也可以手动的把所有节点用静态的方式配置到 Prometheus 中去，但是以后要新增或者去掉节点的时候就还得手动去配置，那么有没有一种方式可以让 Prometheus 去自动发现我们节点的 <code>node-exporter</code> 程序，并且按节点进行分组呢？这就是 Prometheus 里面非常重要的<strong>服务发现</strong>功能了。</p>
<p>在 Kubernetes 下，Promethues 通过与 Kubernetes API 集成，主要支持5中服务发现模式，分别是：<code>Node</code>、<code>Service</code>、<code>Pod</code>、<code>Endpoints</code>、<code>Ingress</code>。</p>
<p>我们通过 kubectl 命令可以很方便的获取到当前集群中的所有节点信息：
<pre class="highlight"><code class="language-shell">$ kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
ydzs-master   Ready    master   33d   v1.16.2
ydzs-node1    Ready    &lt;none&gt;   33d   v1.16.2
ydzs-node2    Ready    &lt;none&gt;   33d   v1.16.2
ydzs-node3    Ready    &lt;none&gt;   31d   v1.16.2
ydzs-node4    Ready    &lt;none&gt;   31d   v1.16.2</code></pre></p>
<p>但是要让 Prometheus 也能够获取到当前集群中的所有节点信息的话，我们就需要利用 Node 的服务发现模式，同样的，在 <code>prometheus.yml</code> 文件中配置如下的 job 任务即可：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-nodes'
  kubernetes_sd_configs:
  - role: node</code></pre></p>
<p>通过指定 <code>kubernetes_sd_configs</code> 的模式为<code>node</code>，Prometheus 就会自动从 Kubernetes 中发现所有的 node 节点并作为当前 job 监控的目标实例，发现的节点 <code>/metrics</code> 接口是默认的 kubelet 的 HTTP 接口。</p>
<p>prometheus 的 ConfigMap 更新完成后，同样的我们执行 reload 操作，让配置生效：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre></p>
<p>配置生效后，我们再去 prometheus 的 dashboard 中查看 Targets 是否能够正常抓取数据，访问<code>http://任意节点IP:30980</code>：</p>
<p><img alt="prometheus webui nodes" src="../../assets/img/monitor/prometheus-webui-nodes.png" /></p>
<p>我们可以看到上面的 <code>kubernetes-nodes</code> 这个 job 任务已经自动发现了我们5个 node 节点，但是在获取数据的时候失败了，出现了类似于下面的错误信息：
<pre class="highlight"><code class="language-shell">server returned HTTP status 400 Bad Request</code></pre></p>
<p>这个是因为 prometheus 去发现 Node 模式的服务的时候，访问的端口默认是10250，而默认是需要认证的 https 协议才有权访问的，但实际上我们并不是希望让去访问10250端口的 <code>/metrics</code> 接口，而是 <code>node-exporter</code> 绑定到节点的 9100 端口，所以我们应该将这里的 <code>10250</code> 替换成 <code>9100</code>，但是应该怎样替换呢？</p>
<p>这里我们就需要使用到 Prometheus 提供的 <code>relabel_configs</code> 中的 <code>replace</code> 能力了，<code>relabel</code> 可以在 Prometheus 采集数据之前，通过 Target 实例的 <code>Metadata</code> 信息，动态重新写入 Label 的值。除此之外，我们还能根据 Target 实例的 <code>Metadata</code> 信息选择是否采集或者忽略该 Target 实例。比如我们这里就可以去匹配 <code>__address__</code> 这个 Label 标签，然后替换掉其中的端口，如果你不知道有哪些 Label 标签可以操作的话，可以将鼠标🎒移动到 Targets 的标签区域，其中显示的 <code>Before relabeling</code> 区域都是我们可以操作的标签：</p>
<p><img alt="prometheus webui relabel before" src="../../assets/img/monitor/prometheus-webui-relabel-before.png" /></p>
<p>现在我们来替换掉端口，修改 ConfigMap：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-nodes'
  kubernetes_sd_configs:
  - role: node
  relabel_configs:
  - source_labels: [__address__]
    regex: '(.*):10250'
    replacement: '${1}:9100'
    target_label: __address__
    action: replace</code></pre></p>
<p>这里就是一个正则表达式，去匹配 <code>__address__</code> 这个标签，然后将 host 部分保留下来，port 替换成了 9100，现在我们重新更新配置文件，执行 reload 操作，然后再去看 Prometheus 的 Dashboard 的 Targets 路径下面 kubernetes-nodes 这个 job 任务是否正常了：</p>
<p><img alt="prometheus webui sd nodes" src="../../assets/img/monitor/prometheus-webui-sd-nodes.png" /></p>
<p>我们可以看到现在已经正常了，但是还有一个问题就是我们采集的指标数据 Label 标签就只有一个节点的 hostname，这对于我们在进行监控分组分类查询的时候带来了很多不方便的地方，要是我们能够将集群中 Node 节点的 Label 标签也能获取到就很好了。这里我们可以通过 <code>labelmap</code> 这个属性来将 Kubernetes 的 Label 标签添加为 Prometheus 的指标数据的标签：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-nodes'
  kubernetes_sd_configs:
  - role: node
  relabel_configs:
  - source_labels: [__address__]
    regex: '(.*):10250'
    replacement: '${1}:9100'
    target_label: __address__
    action: replace
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)</code></pre></p>
<p>添加了一个 action 为 <code>labelmap</code>，正则表达式是 <code>__meta_kubernetes_node_label_(.+)</code> 的配置，这里的意思就是表达式中匹配都的数据也添加到指标数据的 Label 标签中去。</p>
<p>对于 <code>kubernetes_sd_configs</code> 下面可用的元信息标签如下：</p>
<ul>
<li><code>__meta_kubernetes_node_name</code>：节点对象的名称</li>
<li><code>_meta_kubernetes_node_label</code>：节点对象中的每个标签</li>
<li><code>_meta_kubernetes_node_annotation</code>：来自节点对象的每个注释</li>
<li><code>_meta_kubernetes_node_address</code>：每个节点地址类型的第一个地址（如果存在）</li>
</ul>
<p>关于 kubernets_sd_configs 更多信息可以查看官方文档：<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Ckubernetes_sd_config%3E">kubernetes_sd_config</a></p>
<p>另外由于 kubelet 也自带了一些监控指标数据，就上面我们提到的 10250 端口，所以我们这里也把 kubelet 的监控任务也一并配置上：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-kubelet'
  kubernetes_sd_configs:
  - role: node
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)</code></pre></p>
<p>但是这里需要特别注意的是这里必须使用 <code>https</code> 协议访问，这样就必然需要提供证书，我们这里是通过配置 <code>insecure_skip_verify: true</code> 来跳过了证书校验，但是除此之外，要访问集群的资源，还必须要有对应的权限才可以，也就是对应的 ServiceAccount 棒的 权限允许才可以，我们这里部署的 prometheus 关联的 ServiceAccount 对象前面我们已经提到过了，这里我们只需要将 Pod 中自动注入的 <code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code> 和 <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> 文件配置上，就可以获取到对应的权限了。</p>
<p>现在我们再去更新下配置文件，执行 reload 操作，让配置生效，然后访问 Prometheus 的 Dashboard 查看 Targets 路径：</p>
<p><img alt="prometheus webui sd kubelet" src="../../assets/img/monitor/prometheus-webui-sd-kubelet.png" /></p>
<p>现在可以看到我们上面添加的 <code>kubernetes-kubelet</code> 和 <code>kubernetes-nodes</code> 这两个 job 任务都已经配置成功了，而且二者的 Labels 标签都和集群的 node 节点标签保持一致了。</p>
<p>现在我们就可以切换到 Graph 路径下面查看采集的一些指标数据了，比如查询 node_load1 指标：</p>
<p><img alt="prometheus webui node load1" src="../../assets/img/monitor/prometheus-webui-node-load1.png" /></p>
<p>我们可以看到将5个节点对应的 <code>node_load1</code> 指标数据都查询出来了，同样的，我们还可以使用 <code>PromQL</code> 语句来进行更复杂的一些聚合查询操作，还可以根据我们的 Labels 标签对指标数据进行聚合，比如我们这里只查询 <code>ydzs-node3</code> 节点的数据，可以使用表达式 <code>node_load1{instance="ydzs-node3"}</code> 来进行查询：</p>
<p><img alt="prometheus webui node3 load1" src="../../assets/img/monitor/prometheus-webui-node3-load1.png" /></p>
<p>到这里我们就把 Kubernetes 集群节点使用 Prometheus 监控起来了，接下来我们再来和大家学习下怎样监控 Pod 或者 Service 之类的资源对象。</p>
<h2 id="_9">容器监控<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h2>
<p>说到容器监控我们自然会想到 <code>cAdvisor</code>，我们前面也说过cAdvisor已经内置在了 kubelet 组件之中，所以我们不需要单独去安装，<code>cAdvisor</code> 的数据路径为 <code>/api/v1/nodes/&lt;node&gt;/proxy/metrics</code>，同样我们这里使用 node 的服务发现模式，因为每一个节点下面都有 kubelet，自然都有 <code>cAdvisor</code> 采集到的数据指标，配置如下：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-cadvisor'
  kubernetes_sd_configs:
  - role: node
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
  - target_label: __address__
    replacement: kubernetes.default.svc:443
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor</code></pre></p>
<p>上面的配置和我们之前配置 <code>node-exporter</code> 的时候几乎是一样的，区别是我们这里使用了 https 的协议，另外需要注意的是配置了 ca.cart 和 token 这两个文件，这两个文件是 Pod 启动后自动注入进来的，通过这两个文件我们可以在 Pod 中访问 apiserver，比如我们这里的 <code>__address__</code> 不再是 nodeip 了，而是 kubernetes 在集群中的服务地址，然后加上<code>__metrics_path__</code> 的访问路径 <code>/api/v1/nodes/${1}/proxy/metrics/cadvisor</code>，因为我们现在是通过 kubernetes 的 apiserver 地址去进行访问的，现在同样更新下配置，然后查看 Targets 路径：</p>
<p><img alt="prometheus webui cadvisor" src="../../assets/img/monitor/prometheus-webui-cadvisor.png" /></p>
<p>我们可以切换到 Graph 路径下面查询容器相关数据，比如我们这里来查询集群中所有 Pod 的 CPU 使用情况，kubelet 中的 cAdvisor 采集的指标和含义，可以查看 <a href="https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md">Monitoring cAdvisor with Prometheus</a> 说明，其中有一项：
<pre class="highlight"><code>container_cpu_usage_seconds_total   Counter     Cumulative cpu time consumed    seconds</code></pre></p>
<p><code>container_cpu_usage_seconds_total</code> 是容器累计使用的 CPU 时间，用它除以 CPU 的总时间，就可以得到容器的 CPU 使用率了：</p>
<p>首先计算容器的 CPU 占用时间，由于节点上的 CPU 有多个，所以需要将容器在每个 CPU 上占用的时间累加起来，Pod 在 1m 内累积使用的 CPU 时间为：(根据 pod 和 namespace 进行分组查询)
<pre class="highlight"><code>sum(rate(container_cpu_usage_seconds_total{image!="",pod!=""}[1m])) by (namespace, pod)</code></pre></p>
<div class="admonition warning">
<p class="admonition-title">metrics 变化</p>
<p>在 Kubernetes 1.16 版本中移除了 cadvisor metrics 的 pod_name 和 container_name 这两个标签，改成了 pod 和 container。</p>
<p>“Removed cadvisor metric labels pod_name and container_name to match instrumentation guidelines. Any Prometheus queries that match pod_name and container_name labels (e.g. cadvisor or kubelet probe metrics) must be updated to use pod and container instead. (#80376, @ehashman)”</p>
</div>
<p>然后计算 CPU 的总时间，这里的 CPU 数量是容器分配到的 CPU 数量，<code>container_spec_cpu_quota</code> 是容器的 CPU 配额，它的值是容器指定的 <code>CPU 个数 * 100000</code>，所以 Pod 在 1s 内 CPU 的总时间为：Pod 的 CPU 核数 * 1s：
<pre class="highlight"><code>sum(container_spec_cpu_quota{image!="", pod!=""}) by(namespace, pod) / 100000</code></pre></p>
<div class="admonition warning">
<p class="admonition-title">CPU 配额</p>
<p>由于 <code>container_spec_cpu_quota</code> 是容器的 CPU 配额，所以只有配置了 resource-limit CPU 的 Pod 才可以获得该指标数据。</p>
</div>
<p>将上面这两个语句的结果相除，就得到了容器的 CPU 使用率：
<pre class="highlight"><code>(sum(rate(container_cpu_usage_seconds_total{image!="",pod!=""}[1m])) by (namespace, pod))
/
(sum(container_spec_cpu_quota{image!="", pod!=""}) by(namespace, pod) / 100000) * 100</code></pre></p>
<p>在 promethues 里面执行上面的 promQL 语句可以得到下面的结果：</p>
<p><img alt="prometheus cadvisor cpu rate" src="../../assets/img/monitor/prometheus-webui-cadvisor-cpu-rate.png" /></p>
<p>Pod 内存使用率的计算就简单多了，直接用内存实际使用量除以内存限制使用量即可：
<pre class="highlight"><code>sum(container_memory_rss{image!=""}) by(namespace, pod) / sum(container_spec_memory_limit_bytes{image!=""}) by(namespace, pod) * 100 != +inf</code></pre></p>
<p>在 promethues 里面执行上面的 promQL 语句可以得到下面的结果：</p>
<p><img alt="prometheus cadvisor memory rate" src="../../assets/img/monitor/prometheus-webui-cadvisor-memory-rate.png" /></p>
<h2 id="apiserver">监控 apiserver<a class="headerlink" href="#apiserver" title="Permanent link">&para;</a></h2>
<p>apiserver 作为 Kubernetes 最核心的组件，当然他的监控也是非常有必要的，对于 apiserver 的监控我们可以直接通过 kubernetes 的 Service 来获取：
<pre class="highlight"><code class="language-shell">$ kubectl get svc
NAME             TYPE           CLUSTER-IP       EXTERNAL-IP             PORT(S)          AGE
kubernetes       ClusterIP      10.96.0.1        &lt;none&gt;                  443/TCP          33d</code></pre></p>
<p>上面这个 Service 就是我们集群的 apiserver 在集群内部的 Service 地址，要自动发现 Service 类型的服务，我们就需要用到 role 为 Endpoints 的 <code>kubernetes_sd_configs</code>，我们可以在 ConfigMap 对象中添加上一个 Endpoints 类型的服务的监控任务：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-apiservers'
  kubernetes_sd_configs:
  - role: endpoints</code></pre></p>
<p>上面这个任务是定义的一个类型为 endpoints 的 kubernetes_sd_configs ，添加到 Prometheus 的 ConfigMap 的配置文件中，然后更新配置：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre></p>
<p>更新完成后，我们再去查看 Prometheus 的 Dashboard 的 target 页面：</p>
<p><img alt="prometheus webui apiserver" src="../../assets/img/monitor/prometheus-webui-apiserver.png" /></p>
<p>我们可以看到 kubernetes-apiservers 下面出现了很多实例，这是因为这里我们使用的是 Endpoints 类型的服务发现，所以 Prometheus 把所有的 Endpoints 服务都抓取过来了，同样的，上面我们需要的服务名为 <code>kubernetes</code> 这个 apiserver 的服务也在这个列表之中，那么我们应该怎样来过滤出这个服务来呢？还记得前面的 <code>relabel_configs</code> 吗？没错，同样我们需要使用这个配置，只是我们这里不是使用 <code>replace</code> 这个动作了，而是 <code>keep</code>，就是只把符合我们要求的给保留下来，哪些才是符合我们要求的呢？我们可以把鼠标放置在任意一个 target 上，可以查看到<code>Before relabeling</code>里面所有的元数据，比如我们要过滤的服务是 <code>default</code> 这个 namespace 下面，服务名为 <code>kubernetes</code> 的元数据，所以这里我们就可以根据对应的 <code>__meta_kubernetes_namespace</code> 和 <code>__meta_kubernetes_service_name</code> 这两个元数据来 relabel，另外由于 kubernetes 这个服务对应的端口是 443，需要使用 https 协议，所以这里我们需要使用 https 的协议，对应的就需要将 ca 证书配置上，如下所示：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-apiservers'
  kubernetes_sd_configs:
  - role: endpoints
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
    action: keep
    regex: default;kubernetes;https</code></pre></p>
<p>现在重新更新配置文件、重新加载 Prometheus，切换到 Prometheus 的 Targets 路径下查看：</p>
<p><img alt="prometheus apiserver target" src="../../assets/img/monitor/prometheus-webui-apiserver-target.png" /></p>
<p>现在可以看到 <code>kubernetes-apiserver</code> 这个任务下面只有 apiserver 这一个实例了，证明我们的 <code>relabel</code> 是成功的，现在我们切换到 Graph 路径下面查看下采集到的数据，比如查询 apiserver 的总的请求数：</p>
<p><img alt="prometheus apiserver rate" src="../../assets/img/monitor/prometheus-webui-apiserver-rate.png" /></p>
<p>这样我们就完成了对 Kubernetes APIServer 的监控。</p>
<p>另外如果我们要来监控其他系统组件，比如 kube-controller-manager、kube-scheduler 的话应该怎么做呢？由于 apiserver 服务 namespace 在 default 使用默认的 Service kubernetes，而其余组件服务在 kube-system 这个 namespace 下面，如果我们想要来监控这些组件的话，需要手动创建单独的 Service，其中 kube-sheduler 的指标数据端口为 10251，kube-controller-manager 对应的端口为 10252，大家可以尝试下自己来配置下这几个系统组件。</p>
<h2 id="pod">监控 Pod<a class="headerlink" href="#pod" title="Permanent link">&para;</a></h2>
<p>上面的 apiserver 实际上就是一种特殊的 Endpoints，现在我们同样来配置一个任务用来专门发现普通类型的 Endpoint，其实就是 Service 关联的 Pod 列表：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-endpoints'
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
    action: replace
    target_label: __scheme__
    regex: (https?)
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
    action: replace
    target_label: __metrics_path__
    regex: (.+)
  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
    action: replace
    target_label: __address__
    regex: ([^:]+)(?::\d+)?;(\d+)
    replacement: $1:$2
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    action: replace
    target_label: kubernetes_name</code></pre></p>
<p>注意我们这里在 <code>relabel_configs</code> 区域做了大量的配置，特别是第一个保留<code>__meta_kubernetes_service_annotation_prometheus_io_scrape</code> 为 true 的才保留下来，这就是说要想自动发现集群中的 Endpoint，就需要我们在 Service 的 <code>annotation</code> 区域添加 <code>prometheus.io/scrape=true</code> 的声明，现在我们先将上面的配置更新，查看下效果：</p>
<p><img alt="prometheus k8s endpoints" src="../../assets/img/monitor/prometheus-webui-endpoints.png" /></p>
<p>我们可以看到 <code>kubernetes-endpoints</code> 这一个任务下面只发现了两个服务，这是因为我们在 <code>relabel_configs</code> 中过滤了 <code>annotation</code> 有 <code>prometheus.io/scrape=true</code> 的 Service，而现在我们系统中只有这样一个 <code>kube-dns</code> 服务符合要求，该 Service 下面有两个实例，所以出现了两个实例：
<pre class="highlight"><code class="language-shell">$ kubectl get svc kube-dns -n kube-system -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "9153"  # metrics 接口的端口
    prometheus.io/scrape: "true"  # 这个注解可以让prometheus自动发现
  creationTimestamp: "2019-11-08T11:59:50Z"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: KubeDNS
  name: kube-dns
  namespace: kube-system
......</code></pre></p>
<p>现在我们在之前创建的 redis 这个 Service 中添加上 <code>prometheus.io/scrape=true</code> 这个 annotation：(prome-redis.yaml)
<pre class="highlight"><code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: redis
  namespace: kube-mon
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9121"
spec:
  selector:
    app: redis
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
  - name: prom
    port: 9121
    targetPort: 9121</code></pre></p>
<p>由于 redis 服务的 metrics 接口在 9121 这个 redis-exporter 服务上面，所以我们还需要添加一个 <code>prometheus.io/port=9121</code> 这样的 annotations，然后更新这个 Service：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prome-redis.yaml
deployment.apps "redis" unchanged
service "redis" changed</code></pre></p>
<p>更新完成后，去 Prometheus 查看 Targets 路径，可以看到 redis 服务自动出现在了 <code>kubernetes-endpoints</code> 这个任务下面：</p>
<p><img alt="prometheus k8s endpoints redis" src="../../assets/img/monitor/prometheus-webui-endpoints-redis.png" /></p>
<p>这样以后我们有了新的服务，服务本身提供了 <code>/metrics</code> 接口，我们就完全不需要用静态的方式去配置了，到这里我们就可以将之前配置的 redis 的静态配置去掉了。</p>
<!-- ## Blackbox（黑盒）监控
前面我们主要介绍了 Prometheus 下如何进行白盒监控，我们监控主机的资源用量、容器的运行状态、数据库中间件的运行数据、自动发现 Kubernetes 集群中的资源等等，这些都是支持业务和服务的基础设施，通过白盒能够了解其内部的实际运行状态，通过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化。而从完整的监控逻辑的角度，除了大量的应用白盒监控以外，还应该添加适当的 `Blackbox（黑盒）`监控，黑盒监控即以用户的身份测试服务的外部可见性，常见的黑盒监控包括`HTTP 探针`、`TCP 探针` 等用于检测站点或者服务的可访问性，以及访问效率等。

黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。

[Blackbox Exporter](https://github.com/prometheus/blackbox_exporter) 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：`HTTP`、`HTTPS`、`DNS`、`TCP` 以及 `ICMP` 的方式对网络进行探测。

同样首先需要在 Kubernetes 集群中运行 `blackbox-exporter` 服务，同样通过一个 ConfigMap 资源对象来为 Blackbox 提供配置，如下所示：（prome-blackbox.yaml）
<pre class="highlight"><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: blackbox-config
  namespace: kube-mon
data:
  blackbox.yml: |-
    modules:
      http_2xx:  # http 检测模块  Blockbox-Exporter 中所有的探针均是以 Module 的信息进行配置
        prober: http
        timeout: 10s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2"]   
          valid_status_codes: [200]  # 这里最好作一个返回状态码，在grafana作图时，有明示---陈刚注释。
          method: GET
          preferred_ip_protocol: "ip4"
      http_post_2xx: # http post 监测模块
        prober: http
        timeout: 10s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2"]
          method: POST
          preferred_ip_protocol: "ip4"
      tcp_connect:  # TCP 检测模块
        prober: tcp
        timeout: 10s
      dns:  # DNS 检测模块
        prober: dns
        dns:
          transport_protocol: "tcp"  # 默认是 udp
          preferred_ip_protocol: "ip4"  # 默认是 ip6
          query_name: "kubernetes.default.svc.cluster.local"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blackbox
  namespace: kube-mon
spec:
  selector:
    matchLabels:
      app: blackbox
  template:
    metadata:
      labels:
        app: blackbox
    spec:
      containers:
      - image: prom/blackbox-exporter:v0.16.0
        name: blackbox
        args:
        - --config.file=/etc/blackbox_exporter/blackbox.yml # ConfigMap 中的配置文件
        - --log.level=error  # 错误级别控制
        ports:
        - containerPort: 9115
        volumeMounts:
        - name: config
          mountPath: /etc/blackbox_exporter
      volumes:
      - name: config
        configMap:
          name: blackbox-config
---
apiVersion: v1
kind: Service
metadata:
  name: blackbox
  namespace: kube-mon
spec:
  selector:
    app: blackbox
  ports:
  - port: 9115
    targetPort: 9115</code></pre>

直接创建上面的资源清单：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f content/monitor/manifests/install/prome-blackbox.yaml 
configmap/blackbox-config created
deployment.apps/blackbox created
service/blackbox created</code></pre>

然后需要在 Prometheus 的配置文件中加入对 `BlackBox` 的抓取设置，如下所示：
<pre class="highlight"><code class="language-yaml">- job_name: "kubernetes-service-dns"
  metrics_path: /probe # 不是 metrics，是 probe
  params:
    module: [dns] # 使用 DNS 模块
  static_configs:
  - targets:
    - kube-dns.kube-system:53  # 不要省略端口号
  relabel_configs:
  - source_labels: [__address__]
    target_label: __param_target
  - source_labels: [__param_target]
    target_label: instance
  - target_label: __address__
    replacement: blackbox:9115  # 服务地址，和上面的 Service 定义保持一致</code></pre>

首先获取 targets 实例的 `__address__` 值写进 `__param_target`，`__param_<name>` 形式的标签里的 name 和它的值会被添加到发送到黑盒的 http 的 header 的 params 当作键值，例如 `__param_module` 对应 `params` 里的`module`。然后获取 `__param_target` 的值，并覆写到 `instance` 标签中，覆写 Target 实例的 `__address__` 标签值为 `BlockBox Exporter` 实例的访问地址，向 `blackbox:9115` 发送请求获取实例的 metrics 信息。然后更新配置：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre>

打开 Prometheus 的 Target 页面，就会看到 上面定义的 kubernetes-service-dns 任务了：

![prometheus blackbox dns](../assets/img/monitor/prometheus-webui-blackbox-dns.png)

回到 Graph 页面，可以使用 `probe_success{job="kubernetes-service-dns"}` 来查看检测结果，这样就实现了对 DNS 的黑盒监控。

除了 DNS的配置外，上面我们还配置了一个 `http_2xx` 的模块，也就是 HTTP 探针，HTTP 探针是进行黑盒监控时最常用的探针之一，通过 HTTP 探针能够对网站或者 HTTP 服务建立有效的监控，包括其本身的可用性，以及用户体验相关的如响应时间等等。除了能够在服务出现异常的时候及时报警，还能帮助系统管理员分析和优化网站体验。这里我们可以使用他来对 http 服务进行检测。

因为前面已经给 Blackbox 配置了 `http_2xx` 模块，所以这里只需要在 Prometheus 中加入抓取任务，这里我们可以结合前面的 Prometheus 的服务发现功能来做黑盒监控，对于 Service 和 Ingress 类型的服务发现，用来进行黑盒监控是非常合适的，配置如下所示：
<pre class="highlight"><code class="language-yaml">- job_name: 'kubernetes-http-services'
  metrics_path: /probe
  params:
    module: [http_2xx]  # 使用定义的http模块
  kubernetes_sd_configs:
  - role: service  # service 类型的服务发现
  relabel_configs:
  # 只有service的annotation中配置了 prometheus.io/http_probe=true 的才进行发现
  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_http_probe]
    action: keep
    regex: true
  - source_labels: [__address__]
    target_label: __param_target
  - target_label: __address__
    replacement: blackbox:9115
  - source_labels: [__param_target]
    target_label: instance
  - action: labelmap
    regex: __meta_kubernetes_service_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_service_name]
    target_label: kubernetes_name

- job_name: 'kubernetes-ingresses'
  metrics_path: /probe
  params:
    module: [http_2xx]  # 使用定义的http模块
  kubernetes_sd_configs:
  - role: ingress  # ingress 类型的服务发现
  relabel_configs:
  # 只有ingress的annotation中配置了 prometheus.io/http_probe=true的才进行发现
  - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_http_probe]
    action: keep
    regex: true
  - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
    regex: (.+);(.+);(.+)
    replacement: ${1}://${2}${3}
    target_label: __param_target
  - target_label: __address__
    replacement: blackbox:9115
  - source_labels: [__param_target]
    target_label: instance
  - action: labelmap
    regex: __meta_kubernetes_ingress_label_(.+)
  - source_labels: [__meta_kubernetes_namespace]
    target_label: kubernetes_namespace
  - source_labels: [__meta_kubernetes_ingress_name]
    target_label: kubernetes_name</code></pre>

我们结合前面的服务发现功能，通过过滤 `prometheus.io/http_probe=true` 的 Service 和 Ingress 才进行 HTTP 探针类型的黑盒监控，其他配置和上面配置 dns 监控的时候是一致的。然后更新配置：
<pre class="highlight"><code class="language-shell">$ kubectl apply -f prometheus-cm.yaml
configmap/prometheus-config configured
# 隔一会儿执行reload操作
$ curl -X POST "http://10.244.3.174:9090/-/reload"</code></pre>

打开 Prometheus 的 Target 页面，就会看到 上面定义的两个任务了：

![prometheus blackbox service ingress](../assets/img/monitor/prometheus-webui-blackbox-service-ingress.png)

但是现在还没有任何数据，这是因为上面是匹配 `__meta_kubernetes_ingress_annotation_prometheus_io_http_probe` 这个元信息，所以如果我们需要让这两个任务发现的话需要在 Service 或者 Ingress 中配置对应的 annotation：
<pre class="highlight"><code class="language-yaml">annotation:
  prometheus.io/http-probe: "true"</code></pre>

比如在我们自己的一个 Ingress 对象中添加上面这个 annotation：
<pre class="highlight"><code class="language-shell">$  kubectl get ingress fe-trait-ingress -o yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ......
    prometheus.io/http-probe: "true"  # 用于黑盒监控
......
spec:
  rules:
  - host: todo.qikqiak.com
    http:
      paths:
      - backend:
          serviceName: fe
          servicePort: 3000
        path: /app(/|$)(.*)
status:
  loadBalancer: {}</code></pre>

这个时候我们查看到 Ingress 这个任务下面已经有抓取任务了：

![prometheus blackbox ingress](../assets/img/monitor/prometheus-webui-blackbox-ingress.png)

比如现在我们可以使用 `probe_duration_seconds` 来检查监控结果：

![prometheus blackbox ingress query](../assets/img/monitor/prometheus-webui-blackbox-ingress-query.png)

对于 Service 是一样的，当然如果你需要对监控的路径、端口这些做控制，我们可以自己在 relabel_configs 中去做相应的配置，比如我们想对 Service 的黑盒做自定义配置，可以想下面这样配置：
<pre class="highlight"><code class="language-yaml">- source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_service_annotation_prometheus_io_http_probe_port, __meta_kubernetes_service_annotation_prometheus_io_http_probe_path]
  action: replace
  target_label: __param_target
  regex: (.+);(.+);(.+);(.+)
  replacement: $1.$2:$3$4</code></pre>

这样我们就需要在 Service 中配置这样的 annotation 了：
<pre class="highlight"><code class="language-yaml">annotation:
  prometheus.io/http-probe: "true"
  prometheus.io/http-probe-port: "8080"
  prometheus.io/http-probe-path: "/healthz"</code></pre>

这样我们就完成了 HTTP 探针的黑盒监控，除此之外，我们还可以配置 TCP 的监控，上面我们已经配置了这个模块，大家可以自己尝试去配置下。
 -->
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      



<!-- Application footer -->
<footer class="md-footer">

    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
        <div class="md-footer-meta__inner md-grid">

            <!-- Copyright and theme information -->
            <div class="md-footer-copyright">
                
                <div class="md-footer-copyright__highlight">
                    Copyright &copy; 2020 Kubernetes技术栈
                </div>
                
                powered by
                <a href="https://www.k8stech.net" title="Kubernetes技术栈">www.k8stech.net</a>
            </div>

            <!-- Social links -->
            
            
            
        </div>
    </div>
</footer>


    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.8397ff9e.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.1e84347e.min.js"></script>
      
        <script src="../../assets/js/hljs/highlight.pack.js"></script>
      
        <script src="../../assets/js/prism.js"></script>
      
    
  </body>
</html>